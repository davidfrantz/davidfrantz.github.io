<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lower Level | DAVID FRANTZ</title>
    <link>https://davidfrantz.github.io/tags/lower-level/</link>
      <atom:link href="https://davidfrantz.github.io/tags/lower-level/index.xml" rel="self" type="application/rss+xml" />
    <description>Lower Level</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019-2020 David Frantz</copyright><lastBuildDate>Mon, 20 Apr 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://davidfrantz.github.io/img/icon-192.png</url>
      <title>Lower Level</title>
      <link>https://davidfrantz.github.io/tags/lower-level/</link>
    </image>
    
    <item>
      <title>FORCE Tutorial: Level 2 ARD</title>
      <link>https://davidfrantz.github.io/tutorials/force-ard/l2-ard/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-ard/l2-ard/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.2&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains what Analysis Ready Data are, and how to use the &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt; to generate them.&lt;/p&gt;
&lt;h2 id=&#34;what-are-levels&#34;&gt;&lt;strong&gt;What are Levels?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Remote sensing products are grouped in a hierarchical classification scheme. Level 0 data are the measurements taken onboard the satellite - they are not available to users. Level 1 data are radiometrically calibrated and georectified. Level 2 data most notably include some sort of atmospheric correction and probably other corrections like topographic correction. Level 3 data are temporal Level 2 aggregates, e.g. pixel based composites or statistical aggregations like multitemporal averages. Level 4 products are model output (classifications etc.), often derived from multi-temporal or multi-sensor measurements. I am defining Levels 1 and 2 as lower-level products and Levels 3 and above as higher-level products.&lt;/p&gt;
&lt;h2 id=&#34;what-are-analysis-ready-data&#34;&gt;&lt;strong&gt;What are Analysis Ready Data?&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Analysis Ready Data are satellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets (&lt;a href=&#34;http://ceos.org/ard&#34;&gt;CEOS&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Simply put, ARD are readily usable for any application without much further processing. In my opinion, such data need to be well corrected for atmospheric and other effects, have undergone a very good and aggressive cloud screening, are accompanied by pixel-based quality indicators (cloud masks but also other criteria), and are provided in a regular non-overlapping grid system without any redundancy in a single coordinate system (at least on the continental scale) in the form of data cubes.&lt;/p&gt;
&lt;p&gt;Just as a note: Although ARD are a huge step forward for increasing scientific and operational uptake from broader user groups, ARD do not represent the ultimate state of Analysis-Readiness. Thus, FORCE additionally provides means to generate highly Analysis Ready Data (hARD) and even highly Analysis Ready Data plus (hARD+), of which the latter can be directly ingested, analyzed, and interpreted in a GIS without ANY further processing. However, the scope of this tutorial is on plain ARD (don&amp;rsquo;t get me wrong, ARD are great!).&lt;/p&gt;
&lt;h2 id=&#34;force-level-2-processing-system&#34;&gt;&lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt; is capable of generating such ARD.&lt;/p&gt;
&lt;p&gt;In principle, the same algorithm is applied to all supported sensors, although specific processing options are triggered or are only available for some sensors. Essentially, L2PS converts each Level 1 image to ARD specification. This includes three main processing steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cloud and cloud shadow detection&lt;/li&gt;
&lt;li&gt;radiometric correction&lt;/li&gt;
&lt;li&gt;data cubing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For Sentinel-2, two additional options are implemented:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;resolution merging, i.e. increase the spatial resolution of the 20m bands to 10m&lt;/li&gt;
&lt;li&gt;co-registration with Landsat time series&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that most options and corrections can be switched off (e.g. atmospheric correction or data cubing) - this tutorial will however focus on our default parameterization for generating ARD.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-l2ps.jpg&#34; data-caption=&#34;FORCE Level 2 Processing System Workflow&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-l2ps.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    FORCE Level 2 Processing System Workflow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial walks you through the main parts, for more details, please refer to the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/index.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;make-sure-to-have-enough-horsepower&#34;&gt;&lt;strong&gt;Make sure to have enough horsepower&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Generating ARD is compute-heavy. Input and output need quite some disc space. And processes need sufficient RAM. Please do not expect this to work on a weak laptop or similar. It is mainly intended to run on sufficiently powerful servers or virtual machines. There are some ways to make most out of rigs where RAM is an issue (see parallel processing section below), but there is a limit. At least, you will need to have 8-8.5GB of RAM for processing a Sentinel-2 image.&lt;/p&gt;
&lt;h3 id=&#34;recommended-folder-structure&#34;&gt;&lt;strong&gt;Recommended folder structure&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Although not strictly required, I strongly suggest to use separate directories for Level 1 input data, Level 2 output data, parametrization, log files, temporary directory (if input are zip/tar.gz containers) and any other auxiliary data. So, let’s start by creating some directories, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /data/force/level1
mkdir /data/force/level2
mkdir /data/force/param
mkdir /data/force/log
mkdir /data/force/misc
mkdir /data/force/temp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-parameter-file&#34;&gt;&lt;strong&gt;The parameter file&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The core element of L2PS is the parameter file. An empty Level 2 parameter file can be generated with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-parameter /data/force/param LEVEL2 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LEVEL2
An empty parameter file skeleton was written to
  /data/force/param/LEVEL2-skeleton.prm
Note that all parameters need to be given, even though some may not be used
with your specific parameterization.
You should rename the file, e.g. my-first-LEVEL2.prm.
Parameterize according to your needs and run with
force-level2 /data/force/param/my-first-LEVEL2.prm
 or for a single image:
force-l2ps image /data/force/param/my-first-LEVEL2.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trailing &lt;code&gt;1&lt;/code&gt; means that descriptions for every parameter will be included. If you prefer a shorter parameter file, give a &lt;code&gt;0&lt;/code&gt; instead. The descriptions can also be found in the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/param.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s rename the file, and have a look:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mv /data/force/param/LEVEL2-skeleton.prm /data/force/param/l2ps.prm
head /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;++PARAM_LEVEL2_START++

# INPUT/OUTPUT DIRECTORIES
# ------------------------------------------------------------------------
# The file queue specifies, which images are to be processed. The full path
# to the file needs to be given. Do  not  paste  the content of the file queue
# into the parameter file. The file queue is mandatory for force-level2, but
# may be NULL for force-l2ps.
# Type: full file path
FILE_QUEUE = NULL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a simple text file. Lines beginning with &lt;code&gt;#&lt;/code&gt; are comments. All parameters are given in tag and value notation (&lt;code&gt;TAG = VALUE&lt;/code&gt;). The file can be edited with any text editor. However, make sure that you are using Unix End-of-Line &lt;code&gt;\n&lt;/code&gt;. MOST errors are because of parameter files with Windows End-of-Line &lt;code&gt;\r\n&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;parameterization&#34;&gt;&lt;strong&gt;Parameterization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start to parameterize L2PS. Open the file in the text editor of your choice, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vi /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;1-input--output&#34;&gt;&lt;strong&gt;1) Input / Output&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The main input is a file queue that holds the full filepaths to all ingested images. All images that are enqueued (processing flag is &lt;code&gt;QUEUED&lt;/code&gt;) will be processed, all other are ignored. After processing, the flag will be set to &lt;code&gt;DONE&lt;/code&gt;. The &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/&#34;&gt;Sentinel-2 Level 1C tutorial&lt;/a&gt; explains how to use the FORCE Level 1 Archiving Suite (FORCE L1AS) to download, organize, and maintain a clean and consistent Sentinel-2 Level 1 data pool, as well as corresponding data queues needed for the Level 2 processing. There isn&amp;rsquo;t a tutorial for Landsat yet, but it works similarly. Let&amp;rsquo;s assume, we already have downloaded some images, the file queue is set like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FILE_QUEUE = /data/force/level1/queue.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we set the directories for output, logfiles and temporary data. The temp directory is mostly used for temporarily unpacking zip/tar.gz containers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_LEVEL2 = /data/force/level2
DIR_LOG = /data/force/log
DIR_TEMP = /data/force/temp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-digital-elevation-model&#34;&gt;&lt;strong&gt;2) Digital Elevation Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A Digital Elevation model is used to improve cloud and cloud shadow detection, atmospheric correction and to perform the topographic correction. The &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-dem/dem/&#34;&gt;DEM tutorial&lt;/a&gt; explains how to properly prepare a Digital Elevation Model (DEM). Let&amp;rsquo;s assume, we already have prepared the DEM, it is set like this. Make sure to set the nodata value correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FILE_DEM = /data/force/misc/dem/srtm.vrt
DEM_NODATA = -32767
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-datacube-parameters&#34;&gt;&lt;strong&gt;3) Datacube parameters&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The datacube parameters, e.g. resolution, projection, tile size, block size, grid origin etc. are under full user control. As data cubing is an essential concept of FORCE, I highly recommend to read the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-datacube/datacube/&#34;&gt;Datacube tutorial&lt;/a&gt;, which explains what a datacube is, how it is parameterized, how you can find a POI, how to visualize the tiling grid, and how to conveniently display cubed data.&lt;/p&gt;
&lt;p&gt;Our parameter file already has some working defaults. You likely want to adjust them to your needs, but for starters, let&amp;rsquo;s take the default values.&lt;/p&gt;
&lt;h3 id=&#34;4-radiometric-correction&#34;&gt;&lt;strong&gt;4) Radiometric correction&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default parameter file already has all radiometric corrections enabled, and this is the setup I commonly use for generating ARD. This includes atmospheric correction with multiple scattering effects, image-based AOD estimation, topographic correction, adjacency effect correction, and nadir BRDF correction. The only thing that needs to be changed (and only if processing Landsat data) is the parameterization of the water vapor correction. Please see the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-dem/dem/&#34;&gt;Water Vapor Database tutorial&lt;/a&gt; for instructions on how to prepare/download the Water Vapor Database. The directory that contains this database needs to be like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_WVPLUT = /data/force/misc/wvdb
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-cloud-detection&#34;&gt;&lt;strong&gt;5) Cloud detection&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default parameter file already has meaningful values for the cloud correction. I usually don&amp;rsquo;t tweak the Fmask parameters. You can probably change the maximum cloud cover parameters to your liking. The &lt;code&gt;MAX_CLOUD_COVER_FRAME&lt;/code&gt; parameter cancels the processing of images that exceed the given threshold. The processing will be canceled right after cloud detection and thus saves quite some processing time. In my opinion, heavily clouded images are most often of little use, and even if cloud detection flags some pixels as &amp;ldquo;clear&amp;rdquo;, they are usually somewhat contaminated, e.g. in transition zones from clear-sky to cloud.. Therefore, I commonly do not go up to 100%. The &lt;code&gt;MAX_CLOUD_COVER_TILE&lt;/code&gt; parameter is similar, but it works on a per tile basis. It suppresses the output for chips (tiled image) that exceed the given threshold.&lt;/p&gt;
&lt;h3 id=&#34;6-resolution-merge&#34;&gt;&lt;strong&gt;6) Resolution merge&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This parameter defines the method used for improving the spatial resolution of Sentinel-2’s 20m bands to 10m. It defaults to the &lt;a href=&#34;https://ieeexplore.ieee.org/document/7452606&#34;&gt;ImproPhe code&lt;/a&gt;, which is a data fusion option with both decent performance and quality. Let&amp;rsquo;s keep this method, but feel free to try the other options.&lt;/p&gt;
&lt;h3 id=&#34;7-co-registration&#34;&gt;&lt;strong&gt;7) Co-Registration&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since v. 3.0, FORCE is able to perform a co-registration of Sentinel-2 images with Landsat time series. For starters, we will not use this option. A separate tutorial is planned asap that will explain how to do this.&lt;/p&gt;
&lt;h3 id=&#34;8-parallel-processing&#34;&gt;&lt;strong&gt;8) Parallel Processing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;FORCE L2PS uses a nested parallelization strategy. The main parallelization level is multiprocessing: individual images are processed simultaneously (the box in the workflow figure). Each process can additionally use multithreading, which means that each image can be additionally processed parallely. The multiplication of both shouldn&amp;rsquo;t exceed the number of threads your system supports.&lt;/p&gt;
&lt;p&gt;I recommend to use as many processes, and as few threads as possible. However, a mild mix may be beneficial, e.g. 2 threads per process. If processing only a few (or one) image, increase the multithreading ratio accordingly. This can speed up the work significantly. If RAM is too small, inncrease the multithreading ratio accordingly. If there isn&amp;rsquo;t enough RAM to support all processes, some images will fail due to insufficient memory.&lt;/p&gt;
&lt;p&gt;To prevent an I/O jam at startup (by reading / extracting a lot of data simultaneously), a delay (in seconds) might be necessary: a new process waits for some seconds before starting. The necessary delay (or none) is dependent on your system’s architecture (I/O speed etc), on sensor to be processed, and whether packed archives or uncompressed images are given as input.&lt;/p&gt;
&lt;p&gt;Please note that I cannot recommend useful default settings. This is extremely dependent on your rig&amp;rsquo;s setup (# of CPUs, RAM, I/O speed, parallel disc access etc.) and on what exactly you are doing (e.g. Sentinel-2 has higher ressource requirements compared to Landsat, are the input images extracted or still packed in zip/tar.gz containers, enabling/disabling certain processing options have an effect, too).&lt;/p&gt;
&lt;p&gt;Please have a look at these two setups (click to enlarge). The plots illustrate how the work (of processing the same 8 images) is being spread to CPUs and threads, how the delay works, and how the processes consume RAM (highly idealized - actually, the memory footprint varies across runtime).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Good setup&lt;/th&gt;
&lt;th&gt;Bad setup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4 processes with 2 threads each&lt;/td&gt;
&lt;td&gt;8 processes with 1 thread each&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RAM is large enough to support this many processes&lt;/td&gt;
&lt;td&gt;RAM is not large enough to support this many processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-good.jpg&#34; &gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-good.jpg&#34; alt=&#34;&#34; width=&#34;200&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-bad.jpg&#34; &gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-bad.jpg&#34; alt=&#34;&#34; width=&#34;200&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In my case, I am running on a bare-metal Ubuntu server with 32 CPUs / 64 threads, 500GB RAM (way more than sufficient), and a RAID6 HDD file system that is directly attached to the server. Both my Landsat and Sentinel-2 input images are still packed. I am using these parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NPROC = 32
NTHREAD = 2
DELAY = 5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;9-output-options&#34;&gt;&lt;strong&gt;9) Output options&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default output options are already my usual setup for ARD generation. The output files will be stored as compressed GeoTiff images with internal blocks for partial access. Note that metadata are written to the FORCE domain, thus they only show up if you look into all metadata domains, e.g.&lt;/p&gt;
&lt;p&gt;The Bottom-of-Atmosphere reflectance product and the Quality Assurance Information are written by default - and they can&amp;rsquo;t be disabled. I typically generate additional quicklooks (&lt;code&gt;OUTPUT_OVV&lt;/code&gt;). If you want to generate pixel based composites in the next step, you should additionally output the &lt;code&gt;OUTPUT_DST&lt;/code&gt;, &lt;code&gt;OUTPUT_VZN&lt;/code&gt;, and &lt;code&gt;OUTPUT_HOT&lt;/code&gt; products. The &lt;code&gt;OUTPUT_AOD&lt;/code&gt; and &lt;code&gt;OUTPUT_WVP&lt;/code&gt; products are not used by any higher level submodule - they are only useful for validation purposes.&lt;/p&gt;
&lt;h2 id=&#34;processing&#34;&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Once the parameter file is finished, processing is pretty straight forward. Simply feed the parameter file to &lt;code&gt;force-level2&lt;/code&gt;. A progress bar keeps you updated about the ETA, the number of completed, running and waiting processes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level2 /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;47 images enqueued. Start processing with 32 CPUs

Computers / CPU cores / Max jobs to run
1:local / 64 / 32

Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
ETA: 12046s Left: 45 AVG: 280.00s  local:32/2/100%/596.5s 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;logfile&#34;&gt;&lt;strong&gt;Logfile&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After processing, I recommend to check the logfiles, which we have written to &lt;code&gt;/data/force/log&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/force/log | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_OPER_MSI_L1C_TL_SGS__20160310T160000_A003736_T33JYG_N02.01
S2A_OPER_MSI_L1C_TL_SGS__20160409T141153_A004165_T33JYG_N02.01
S2A_OPER_MSI_L1C_TL_SGS__20160827T135818_A006167_T33JYG_N02.04
S2A_OPER_MSI_L1C_TL_SGS__20160916T135429_A006453_T33JYG_N02.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logfiles report the percentage of data cover (how many pixels are not no-data), water cover, snow cover and cloud cover. Then, aerosol optical depth @ 550 nm (scene average), and the number of dark targets for retrieving aerosol optical depth (over water/vegetation) are printed. Then, the number of products written (number of tiles), and a supportive success indication is printed. In the case the overall cloud coverage is higher than allowed, the image is skipped. The processing time (real time) is appended at the end.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/force/log/* | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_OPER_MSI_L1C_TL_SGS__20160310T160000_A003736_T33JYG_N02.01: dc:  99.95%. wc:  18.04%. sc:   0.07%. cc:   7.13%. AOD: 0.1129. # of targets: 240/42. 25 product(s) written. Success! Processing time: 14 mins 35 secs
S2A_OPER_MSI_L1C_TL_SGS__20160409T141153_A004165_T33JYG_N02.01: dc: 100.00%. wc:  18.39%. sc:   0.07%. cc:   0.08%. AOD: 0.1455. # of targets: 25/43. 25 product(s) written. Success! Processing time: 15 mins 19 secs
S2A_OPER_MSI_L1C_TL_SGS__20160827T135818_A006167_T33JYG_N02.04: dc: 100.00%. wc:  18.43%. sc:   0.10%. cc:   0.11%. AOD: 0.1208. # of targets: 74/0. 25 product(s) written. Success! Processing time: 13 mins 50 secs
S2A_OPER_MSI_L1C_TL_SGS__20160916T135429_A006453_T33JYG_N02.04: dc: 100.00%. wc:   1.78%. sc:   2.85%. cc: 100.00%. Skip. Processing time: 12 mins 17 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;output-format&#34;&gt;&lt;strong&gt;Output format&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For all details on the output format, please refer to the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/format.html#metadata&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The output data are organized in data cubes. The tiles manifest as directories in the file system, and the images are stored within. This is decribed in more detail in the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-datacube/datacube/&#34;&gt;Datacube tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically, for each tile, you get a time series of square image chips that always show the same extent:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-results.jpg&#34; data-caption=&#34;Data Cube of Landsat 7/8 and Sentinel-2 A/B Level 2 ARD. A two-month period of atmospherically corrected imagery acquired over South-East Berlin, Germany, is shown here.&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-results.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Data Cube of Landsat 7/8 and Sentinel-2 A/B Level 2 ARD. A two-month period of atmospherically corrected imagery acquired over South-East Berlin, Germany, is shown here.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Each dataset consists of a &lt;code&gt;BOA&lt;/code&gt; and &lt;code&gt;QAI&lt;/code&gt; product, which are Bottom-of-Atmosphere reflectance and Quality Assurance Information. Depending on parameterization ,there are more products, e.g. &lt;code&gt;OVV&lt;/code&gt; for image overviews (quicklooks).&lt;/p&gt;
&lt;p&gt;The reflectance products are multi-band images and consist of 6 bands for Landsat (Landsat legacy bands), and 10 bands for Sentinel-2 (land surface bands). All bands are provided at the same spatial resolution, typically 30m for Landsat and 10m for Sentinel-2.&lt;/p&gt;
&lt;p&gt;QAI are provided bit-wise for each pixel. QAI are essential for making your analyses a success, therefore, please have a look at the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-qai/qai/&#34;&gt;Quality Bits tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Metadata are written to all output products. Note that FORCE-specific metadata will be written to the FORCE domain, and thus are probably not visible unless the FORCE domain (or all domains) are specifically requested:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{bash,&#34;&gt;gdalinfo -mdd all /data/force/level2/X0007_Y0007/20170424_LEVEL2_SEN2A_BOA.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Driver: GTiff/GeoTIFF
Files: /data/force/level2/X0007_Y0007/20170424_LEVEL2_SEN2A_BOA.tif
Size is 3000, 3000
Coordinate System is:
PROJCS[&amp;quot;WGS 84 / UTM zone 33S&amp;quot;,
    GEOGCS[&amp;quot;WGS 84&amp;quot;,
        DATUM[&amp;quot;WGS_1984&amp;quot;,
        
...

Band 10 Block=3000x300 Type=Int16, ColorInterp=Undefined
  Description = SWIR2
  NoData Value=-9999
  Metadata (FORCE):
    Date=2017-04-24T08:26:01.0Z
    Domain=SWIR2
    Scale=10000.000
    Sensor=SEN2A
    Wavelength=2.202
    Wavelength_unit=micrometers
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ard-now-what&#34;&gt;&lt;strong&gt;ARD, now what?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;FORCE provides a lot of functionality to further process the generated ARD into hARD or hARD+ products, e.g. using pixel-based compositing or time series analyses.&lt;/p&gt;
&lt;p&gt;Please see the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/index.html&#34;&gt;Higher Level processing options&lt;/a&gt; in the documentation. Some more tutorials are planned, which deal with all these options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Sentinel-2 Level 1C</title>
      <link>https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/</link>
      <pubDate>Sat, 15 Feb 2020 15:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains how to use the FORCE Level 1 Archiving Suite (FORCE L1AS) to download, organize, and maintain a clean and consistent Sentinel-2 Level 1 data pool, as well as corresponding data queues needed for the Level 2 processing.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ESA provides an application programming interface (API) for data query and automatic download (see &lt;a href=&#34;https://scihub.copernicus.eu/twiki/do/view/SciHubUserGuide/BatchScripting?redirectedfrom=SciHubUserGuide.8BatchScripting&#34;&gt;here&lt;/a&gt;). Based on some user-defined parameters (coordinates etc.) &lt;strong&gt;FORCE L1AS&lt;/strong&gt; pulls a metadata report from the Copernicus API Hub. Each hit is compared
with the local data holdings you already downloaded. If a new file is sitting on ESA&amp;rsquo;s end, the missing image is downloaded. A file queue is generated and updated accordingly - which is the main input to the &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt;.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l1sen2.png&#34; data-caption=&#34;Sentinel-2 downloader in the FORCE Level 1 Archiving Suite&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l1sen2.png&#34; alt=&#34;&#34; width=&#34;750&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sentinel-2 downloader in the &lt;strong&gt;FORCE Level 1 Archiving Suite&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;setup&#34;&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Now, the first step is to get access to ESA&amp;rsquo;s data and services. For that, you need an account. If you don&amp;rsquo;t have one, register &lt;a href=&#34;https://scihub.copernicus.eu/dhus/#/self-registration&#34;&gt;here&lt;/a&gt;. It&amp;rsquo;s free.&lt;/p&gt;
&lt;p&gt;Then, your credentials must be made available to &lt;strong&gt;FORCE L1AS&lt;/strong&gt; to be able to request and receive ESA data. On your machine, your login credentials must be placed in a hidden file &lt;code&gt;.scihub&lt;/code&gt; in your home directory. I advise to only give user reading rights to this file. The user name goes in the first line, the password in the second line. Please note that special characters might be problematic. Also note, if you generate this file from a Windows machine, the Windows EOL character will cause problems.&lt;/p&gt;
&lt;h2 id=&#34;download-some-data&#34;&gt;&lt;strong&gt;Download some data&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;instructions&#34;&gt;&lt;strong&gt;Instructions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;After setting up your account, you should be able to download Sentinel-2 data via &lt;strong&gt;FORCE L1AS&lt;/strong&gt;. As with any other &lt;strong&gt;FORCE&lt;/strong&gt; program, you can display short usage instructions by executing the program without any parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-level1-sentinel2 Level-1-Datapool queue Boundingbox
                   starttime endtime min-cc max-cc [dry]

  Level-1-Datapool
  An existing directory, your files will be stored here

  queue
  Downloaded files are appended to a file queue, which is needed for
  the Level 2 processing. The file doesn&#39;t need to exist. If it exists,
  new lines will be appended on successful ingestion

  Boundingbox
  The coordinates of your study area: &amp;quot;X1/Y1,X2/Y2,X3/Y3,...,X1/Y1&amp;quot;
  The box must be closed (first X/Y = last X/Y). X/Y must be given as
  decimal degrees with negative values for West and South coordinates.
  Note that the box doesn&#39;t have to be square, you can specify a polygon

  starttime endtime
  Dates must be given as YYYY-MM-DD

  min-cc max-cc
  The cloud cover range must be given in %

  dry will trigger a dry run that will only return the number of images
  and their total data volume

  Your ESA credentials must be placed in /home/frantzda/.scihub
    First line: User name
    Second line: Password, special characters might be problematic
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dry-run&#34;&gt;&lt;strong&gt;Dry run&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Please note that &lt;strong&gt;FORCE&lt;/strong&gt; won&amp;rsquo;t check that there is enough space on your hard disc. If you don&amp;rsquo;t dare to download all data straight away, there is a dry run option implemented that only checks how much data would be downloaded with the parameters you provided. This is given by the optional &lt;code&gt;dry&lt;/code&gt; keyword at the end of the command line. The following query asks for all data in July 2019 with a maximum cloud coverage of 50%. The region of interest here is a rather small area in Zambia&amp;rsquo;s Northwestern Province, depicting a large copper mine in the Miombo forest.&lt;/p&gt;
&lt;p&gt;The single most frequent error here is the specification of the coordinates. If you don&amp;rsquo;t receive any data for your study area, or end up somewhere else entirely, double check that the coordinates are given in the correct order.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X = Longitude&lt;/li&gt;
&lt;li&gt;Y = Latitude&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50 dry
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:36:36 - Found 13 S2A/B files.
13 Sentinel-2 A/B L1C files available
5.19094 GB data volume available
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;download&#34;&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The actual download is triggered by omitting the &lt;code&gt;dry&lt;/code&gt; option. &lt;strong&gt;FORCE L1AS&lt;/strong&gt; downloads all data that match the parameters provided - and which weren&amp;rsquo;t downloaded before. Note that the program checks against the files on the disc (not the file queue). Each downloaded image is unzipped after the download. If both steps were successful, the image is appended to the file queue.&lt;/p&gt;
&lt;p&gt;Do not wonder if FORCE tells you that it has found exactly 100 S2A/B files. The ESA API Hub only allows to retrieve metadata for 100 products. Thus, FORCE iterates through the pages until no more image can be retrieved.&lt;/p&gt;
&lt;p&gt;Please note that download speed varies considerably..&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:37:03 - Found 13 S2A/B files.
2020-02-15_15:37:03 - Found 13 S2A/B files on this page.
/data/Dagobah 100%[===================&amp;gt;] 729.50M  32.4MB/s    in 23s     
/data/Dagobah 100%[===================&amp;gt;] 271.14M  30.5MB/s    in 7.0s    
/data/Dagobah 100%[===================&amp;gt;] 742.98M  29.8MB/s    in 24s     
/data/Dagobah 100%[===================&amp;gt;] 266.53M  28.1MB/s    in 11s     
/data/Dagobah 100%[===================&amp;gt;] 732.80M  30.7MB/s    in 20s     
/data/Dagobah 100%[===================&amp;gt;] 224.77M  69.2MB/s    in 3.2s    
/data/Dagobah 100%[===================&amp;gt;] 730.90M  81.6MB/s    in 9.6s    
/data/Dagobah 100%[===================&amp;gt;] 268.45M  42.5MB/s    in 7.9s    
/data/Dagobah 100%[===================&amp;gt;] 704.98M  45.8MB/s    in 21s     
/data/Dagobah 100%[===================&amp;gt;] 258.02M  47.5MB/s    in 5.9s    
/data/Dagobah 100%[===================&amp;gt;] 754.09M  61.7MB/s    in 13s     
/data/Dagobah 100%[===================&amp;gt;] 259.80M  66.5MB/s    in 4.3s    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the Level 1 Datapool, there is now the file queue, as well as a directory for each MGRS tile that was retrieved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;T35LLG  zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the directories, there are the unzipped images.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C/T*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that for Sentinel-2, the compression is realized in the image data, not in the container, thus unzipping the data does not inflate the file size (much).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;du -h -d 1 /data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.9G	/data/Dagobah/S2L1C/T35LLG
5.9G	/data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file queue is holding the full filepaths to all ingested images. This is the main input to &lt;strong&gt;force-level2&lt;/strong&gt;. Each image is in a separate line. A processing-state flag determines if the image is enqueued for Level 2 processing - or was already processed and will be ignored next time. This flag is either &lt;code&gt;QUEUED&lt;/code&gt; or &lt;code&gt;DONE&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/S2L1C/zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE QUEUED
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;download-some-more-data&#34;&gt;&lt;strong&gt;Download some more data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Downloading more data is easy. You can use the same datapool, and the same file queue for this. Images are only downloaded if they weren&amp;rsquo;t downloaded yet. Thus, you can e.g. change the boundingbox, time frame or cloud coverage, and only download data that was not covered by the previous run. In the following example, the Eastern X-Coordinates were increased by 1 degree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,26.94/-12.46,26.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:43:57 - Found 26 S2A/B files.
2020-02-15_15:43:57 - Found 26 S2A/B files on this page.
/data/Dagobah 100%[===================&amp;gt;]  54.08M  43.4MB/s    in 1.2s    
/data/Dagobah 100%[===================&amp;gt;] 794.27M  73.7MB/s    in 11s     
/data/Dagobah 100%[===================&amp;gt;]  57.66M  61.0MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 782.06M  80.1MB/s    in 10s     
/data/Dagobah 100%[===================&amp;gt;]  49.95M  52.0MB/s    in 1.0s    
/data/Dagobah 100%[===================&amp;gt;] 555.54M  85.9MB/s    in 6.8s    
/data/Dagobah 100%[===================&amp;gt;]  52.83M  57.7MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 788.67M  79.2MB/s    in 12s     
/data/Dagobah 100%[===================&amp;gt;]  48.47M  52.4MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 779.62M  81.5MB/s    in 9.4s    
/data/Dagobah 100%[===================&amp;gt;]  58.56M  50.9MB/s    in 1.1s    
/data/Dagobah 100%[===================&amp;gt;] 781.21M  54.6MB/s    in 17s     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the 26 available images, only 13 were retrieved, 13 were already downloaded before. There are now several directories with different MGRS tiles in your Level 1 Datapool.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C/T*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG:
S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE

/data/Dagobah/S2L1C/T35LMG:
S2A_MSIL1C_20190707T080611_N0207_R078_T35LMG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LMG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LMG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LMG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LMG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LMG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LMG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LMG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LMG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LMG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LMG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LMG_20190725T120407.SAFE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new files were appended to the file queue, too.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/S2L1C/zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LMG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LMG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LMG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LMG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LMG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LMG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LMG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LMG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LMG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LMG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LMG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LMG_20190702T115117.SAFE QUEUED
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setting-up-a-scheduled-download&#34;&gt;&lt;strong&gt;Setting up a scheduled download&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The same logic can be used to set up a scheduler for downloading your data at regular intervals. For example, a daily cronjob can be installed to retrieve all data covering your study area. A cronjob is installed by adding lines to the cronjob file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crontab -e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your FORCE installation is not in a standard search path, you need to define the PATH variable, and include the path where FORCE is installed. Then, use the command line from above and schedule it with cron notation. Following line will start the download at 3:00 AM each day. &lt;em&gt;Replace YOURNAME with your user name.&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PATH=/home/YOURNAME/bin:/usr/bin:/bin:/usr/local/bin
0 3 * * * force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,26.94/-12.46,26.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2018-07-01 2018-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;long-term-archive&#34;&gt;&lt;strong&gt;Long Term Archive&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In September 2019, ESA has activated the Long Term Archive (LTA) to roll out old (and potentially infrequently used) data products from the online storage system to offline storage. For details, see &lt;a href=&#34;https://scihub.copernicus.eu/userguide/LongTermArchive&#34;&gt;here&lt;/a&gt;. As of now (the following numbers might change in the future), the last year of data shall stay online, and is immediately ready for download. Offline data may be pulled from offline to online storage upon request. The data retrieval shall happen within 24h and the products shall stay online for 3 days. If they were not downloaded within this time period, they need to be pulled again. A user quota is implemented to prevent users from pulling the entire archive - unfortunately this quota is ridicously low, 1 request per hour per user&amp;hellip; Let&amp;rsquo;s all hope it doesn&amp;rsquo;t stay this way :/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FORCE &amp;gt;= 3.0&lt;/strong&gt; is able to handle LTA data. Previous &lt;strong&gt;FORCE&lt;/strong&gt; versions crash when trying to download offline products.
&lt;strong&gt;FORCE L1AS&lt;/strong&gt; determines whether a product is online or offline.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If online, the image is downloaded as described above.&lt;/li&gt;
&lt;li&gt;If offline, a pull request from offline to online storage is sent. ESA hasn&amp;rsquo;t implemented any callback for this retrieval, thus &lt;strong&gt;FORCE L1AS&lt;/strong&gt; will simply send pull requests for each requested offline image, probably download some available online images, and then exit. &lt;strong&gt;FORCE L1AS&lt;/strong&gt; needs to be run again to retrieve the restored data. To this end, it comes in handy to set up a download scheduler as desribed above.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2018-07-01 2018-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:49:18 - Found 12 S2A/B files.
2020-02-15_15:49:18 - Found 12 S2A/B files on this page.
S2B_MSIL1C_20180730T081559_N0206_R121_T35LLG_20180730T141111.SAFE: Pulling from Long Term Archive. Success. Rerun this program after a while
S2B_MSIL1C_20180727T080609_N0206_R078_T35LLG_20180727T121446.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180725T081601_N0206_R121_T35LLG_20180725T121615.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180722T080611_N0206_R078_T35LLG_20180722T115605.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2B_MSIL1C_20180720T081559_N0206_R121_T35LLG_20180720T121127.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2B_MSIL1C_20180717T080609_N0206_R078_T35LLG_20180717T120239.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180715T081601_N0206_R121_T35LLG_20180715T103432.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180712T080611_N0206_R078_T35LLG_20180712T102334.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2B_MSIL1C_20180710T081559_N0206_R121_T35LLG_20180710T120813.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2B_MSIL1C_20180707T080609_N0206_R078_T35LLG_20180707T115209.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2A_MSIL1C_20180705T081601_N0206_R121_T35LLG_20180705T103349.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2A_MSIL1C_20180702T080611_N0206_R078_T35LLG_20180702T115230.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: DEM</title>
      <link>https://davidfrantz.github.io/tutorials/force-dem/dem/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-dem/dem/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will show how to prepare a Digital Elevation Model (DEM) for the &lt;strong&gt;FORCE Level 2 Processing System (FORCE L2PS)&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;FORCE L2PS&lt;/strong&gt; uses a DEM for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enhanced cloud and cloud shadow detection,&lt;/li&gt;
&lt;li&gt;atmospheric correction, and to&lt;/li&gt;
&lt;li&gt;perform the topographic correction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the cloud shadow detection, the DEM is primarily used to distinguish cloud shadows from water and topographic shadows. In the atmospheric correction, the DEM is used to scale the optical depths with altitude. The topographic correction is of course relying on the DEM. In principle, &lt;strong&gt;FORCE L2PS&lt;/strong&gt; can be used without a DEM (FILE_DEM = NULL). In this case, the surface is assumed to be flat at z = 0m a.s.l. The topographic correction, however, can only be used if a DEM is given (surprise).&lt;/p&gt;
&lt;p&gt;In any case, it is strongly advised to use a DEM. Plus, it is not complicated to acquire it, free options are available. You probably already have a DEM for your study area anyway.&lt;/p&gt;
&lt;h2 id=&#34;data-format&#34;&gt;&lt;strong&gt;Data format&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;There are little requirements on the data format:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The unit must be meters.&lt;/li&gt;
&lt;li&gt;The Nodata value shouldn&amp;rsquo;t be 0, which is a valid elevation.&lt;/li&gt;
&lt;li&gt;The DEM must cover the complete image(s) to be processed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, a mosaic that covers your complete study area needs to be prepared. The DEM is warped and cropped to the projection and extent of the Level 1 image, which is processed with &lt;strong&gt;FORCE L2PS&lt;/strong&gt;. This is done on-the-fly. Therefore, data type, data format, projection, extent etc. can be chosen freely - as long as GDAL is able to handle it (GDAL can handle pretty much anything).&lt;/p&gt;
&lt;p&gt;Please note, that pixels with nodata values in the DEM will have nodata values in the Level 2 products, too. Thus, make sure your DEM covers the complete area of interest.&lt;/p&gt;
&lt;h2 id=&#34;which-dem&#34;&gt;&lt;strong&gt;Which DEM?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The DEM should match the resolution of the Level 1 image data as closely as possible. If possible, it is advised to use a finer resolution. However, as it is hard to acquire high spatial resolution DEMs, especially for larger areas, lower resolution works too. Often, we use the 30m SRTM DEM or 30m ASTER DEM, or a combination thereof, e.g. SRTM filled with ASTER (SRTM is a bit better, but there are holes in mountainous regions, and coverage is only 60°N-60°S).&lt;/p&gt;
&lt;p&gt;The SRTM DEM can be obtained from &lt;a href=&#34;https://earthexplorer.usgs.gov/&#34;&gt;EarthExplorer&lt;/a&gt;. The ASTER DEM can be obtained from &lt;a href=&#34;https://search.earthdata.nasa.gov/search/&#34;&gt;EarthData&lt;/a&gt; or &lt;a href=&#34;https://ssl.jspacesystems.or.jp/ersdac/GDEM/E/&#34;&gt;Japan Space Systems&lt;/a&gt;. Both are free of charge.&lt;/p&gt;
&lt;h2 id=&#34;prepare-the-mosaic&#34;&gt;&lt;strong&gt;Prepare the mosaic&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The following steps illustrate how to build a virtual mosaic from SRTM data. Generally, DEM data come in tiles (datacube style), e.g. each SRTM tile covers 1°. The &lt;a href=&#34;gdal.org/drivers/raster/vrt.html&#34;&gt;GDAL Virtual Format&lt;/a&gt; allows to mosaick data without producing a physical representation, i.e. the virtual mosaic only holds links to the original tiled data, plus some rules on how to combine them into the mosaic.&lt;/p&gt;
&lt;p&gt;Assuming you have downloaded some SRTM tiles, we first prepare a text file that holds all the filepaths:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find /data/Dagobah/global/dem/srtm -name &#39;*.tif&#39; &amp;gt; /data/Earth/global/dem/srtm.txt
cat /data/Dagobah/global/dem/srtm.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/global/dem/srtm/n35_e027_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e027_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e027_1arc_v3.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use the &lt;code&gt;gdalbuildvrt&lt;/code&gt; command to generate the virtual mosaic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalbuildvrt -input_file_list /data/Dagobah/global/dem/srtm.txt /data/Earth/global/dem/srtm.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The VRT file is a simple xml file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head -n 14 /data/Dagobah/global/dem/srtm.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;VRTDataset rasterXSize=&amp;quot;18001&amp;quot; rasterYSize=&amp;quot;10801&amp;quot;&amp;gt;
  &amp;lt;SRS&amp;gt;GEOGCS[&amp;quot;WGS 84&amp;quot;,DATUM[&amp;quot;WGS_1984&amp;quot;,SPHEROID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7030&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6326&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4326&amp;quot;]]&amp;lt;/SRS&amp;gt;
  &amp;lt;GeoTransform&amp;gt;  2.2999861111111112e+01,  2.7777777777777794e-04,  0.0000000000000000e+00,  3.8000138888888891e+01,  0.0000000000000000e+00, -2.7777777777777794e-04&amp;lt;/GeoTransform&amp;gt;
  &amp;lt;VRTRasterBand dataType=&amp;quot;Int16&amp;quot; band=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;NoDataValue&amp;gt;-32767&amp;lt;/NoDataValue&amp;gt;
    &amp;lt;ColorInterp&amp;gt;Gray&amp;lt;/ColorInterp&amp;gt;
    &amp;lt;ComplexSource&amp;gt;
      &amp;lt;SourceFilename relativeToVRT=&amp;quot;1&amp;quot;&amp;gt;srtm/n35_e027_1arc_v3.tif&amp;lt;/SourceFilename&amp;gt;
      &amp;lt;SourceBand&amp;gt;1&amp;lt;/SourceBand&amp;gt;
      &amp;lt;SourceProperties RasterXSize=&amp;quot;3601&amp;quot; RasterYSize=&amp;quot;3601&amp;quot; DataType=&amp;quot;Int16&amp;quot; BlockXSize=&amp;quot;3601&amp;quot; BlockYSize=&amp;quot;1&amp;quot; /&amp;gt;
      &amp;lt;SrcRect xOff=&amp;quot;0&amp;quot; yOff=&amp;quot;0&amp;quot; xSize=&amp;quot;3601&amp;quot; ySize=&amp;quot;3601&amp;quot; /&amp;gt;
      &amp;lt;DstRect xOff=&amp;quot;14400&amp;quot; yOff=&amp;quot;7200&amp;quot; xSize=&amp;quot;3601&amp;quot; ySize=&amp;quot;3601&amp;quot; /&amp;gt;
      &amp;lt;NODATA&amp;gt;-32767&amp;lt;/NODATA&amp;gt;
    &amp;lt;/ComplexSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any software that is based on GDAL is able to read this file, e.g. QGIS - and &lt;strong&gt;FORCE&lt;/strong&gt;. The filepath of this file needs to given in the &lt;strong&gt;FORCE L2PS&lt;/strong&gt; parameter file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FILE_DEM = /data/Dagobah/global/dem/srtm.vrt&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: The Datacube</title>
      <link>https://davidfrantz.github.io/tutorials/force-datacube/datacube/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-datacube/datacube/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;script src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34; type=&#34;text/javascript&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains what a datacube is, how it is parameterized, how you can find a POI, how to visualize the tiling grid, and how to conveniently display cubed data.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;FORCE makes heavy use of the data cube concept. This includes two main points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All data are in the &lt;strong&gt;same coordinate system&lt;/strong&gt;, which should be valid for a large regional extent (e.g. a continental projection).&lt;/li&gt;
&lt;li&gt;The data are organized in regular, non-overlapping &lt;strong&gt;tiles&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-scheme.jpg&#34; data-caption=&#34;Overview of the datacube concept in FORCE&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-scheme.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overview of the datacube concept in &lt;strong&gt;FORCE&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The ‘grid’ is the regular spatial subdivision of the land surface in the target coordinate system.&lt;/li&gt;
&lt;li&gt;The ‘grid origin’ is the location, where the tile numbering starts with zero. Tile numbers increase toward the South and East. Although not recommended, negative tile numbers may be present if the tile origin is not North–West of the study area.&lt;/li&gt;
&lt;li&gt;The ‘tile’ is one entity of the grid, i.e., a grid cell with a unique tile identifier, e.g., X0003_Y0002. The tile is stationary, i.e., it always covers the same extent on the land surface.&lt;/li&gt;
&lt;li&gt;The ‘tile size’ is defined in target coordinate system units (most commonly in meters). Tiles are square.&lt;/li&gt;
&lt;li&gt;Each ‘original image’ is partitioned into several ‘chips’, i.e., any original image is intersected with the grid and then tiled into chips.&lt;/li&gt;
&lt;li&gt;Chips are grouped in ‘datasets’, which group data, e.g. according to acquisition date and sensor.&lt;/li&gt;
&lt;li&gt;The ‘data cube’ groups all datasets within a tile in a time-ordered manner. The data cube may contain data from several sensors and different resolutions. Thus, the pixel size is allowed to vary, but the tile extent stays fixed. The tile size must be a multiple of the resolutions. Other data like features or auxiliary data are also permitted in the data cube (e.g. DEM or climate variables).&lt;/li&gt;
&lt;li&gt;The data cube concept allows for non-redundant data storage and efficient data access, as well as simplified extraction of data and information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;how-to-define-the-datacube-parameters&#34;&gt;&lt;strong&gt;How to define the datacube parameters?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;When generating Level 2 ARD data with &lt;strong&gt;FORCE L2PS&lt;/strong&gt;, you need to define the datacube in the parameter file. Empty parameter files can be generated with &lt;code&gt;force-parameter&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DO_REPROJ&lt;/code&gt; indicates whether the images should be reprojected to the target coordinate system - or stay in their original UTM projection.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DO_TILE&lt;/code&gt; indicates whether the images should be tiled to chips that intersect with the grid system - or stay in the original reference system (WRS-2/MGRS).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PROJECTION&lt;/code&gt; defines the target coordinate system. This projection should ideally be valid for a large geographic extent. The projection needs to given as &amp;ldquo;WKT&amp;rdquo; string. You can verify your projection (and convert to WKT from another format) using &lt;code&gt;gdalsrsinfo&lt;/code&gt; (see below). If this fails, you need to fix the projection - otherwise &lt;strong&gt;FORCE L2PS&lt;/strong&gt; will likely fail, too.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ORIGIN_LAT&lt;/code&gt; and &lt;code&gt;ORIGIN_LON&lt;/code&gt; are the origin coordinates of the grid system in decimal degree. The upper left corner of tile X0000_Y0000 represents this point. It is a good choice to use a coordinate that is North-West of your study area – to avoid negative tile numbers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TILE_SIZE&lt;/code&gt; is the tile size (in target units, commonly in meters). Tiles are square.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BLOCK_SIZE&lt;/code&gt; is the block size (in target units, commonly in meters) of the image chips. Blocks are stripes, i.e. they are as wide as the tile and as high as specified here. The blocks represent the internal structure of the GeoTiffs, and represent the primary processing unit of the force-higher-level routines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two default projection / grid systems are predefined in &lt;strong&gt;FORCE&lt;/strong&gt;. They can be specified via the &lt;code&gt;PROJECTION&lt;/code&gt; parameter instead of giving a WKT string. The predefined options have their own settings for &lt;code&gt;ORIGIN_LAT&lt;/code&gt;, &lt;code&gt;ORIGIN_LON&lt;/code&gt;, &lt;code&gt;TILE_SIZE&lt;/code&gt;, and &lt;code&gt;BLOCK_SIZE&lt;/code&gt;, thus the values given in the parameterfile will be ignored. &lt;a href=&#34;https://cartography.tuwien.ac.at/eurocarto/wp-content/uploads/2015/09/3_6_ppt.pdf&#34;&gt;EQUI7&lt;/a&gt; consists of 7 Equi-Distant, continental projections with a tile size of 100km. &lt;a href=&#34;https://measures-glance.github.io/glance-grids/&#34;&gt;GLANCE7&lt;/a&gt; consists of 7 Equal-Area, continental projections, with a tile size of 150km. One datacube will be generated for each continent.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you are not using the datacube options, i.e. &lt;code&gt;DO_REPROJ = FALSE&lt;/code&gt; or &lt;code&gt;DO_TILE = FALSE&lt;/code&gt;, you are running into a &lt;strong&gt;dead end&lt;/strong&gt; for &lt;strong&gt;FORCE&lt;/strong&gt;. In this case, the data cannot be further processed or analysed with any higher level FORCE functionality&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-to-validate-the-projection&#34;&gt;&lt;strong&gt;How to validate the projection?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GDAL has a built-in projection conversion/validation tool:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalsrsinfo -v &#39;PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,GEOGCS[&amp;quot;ETRS89&amp;quot;,DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],UNIT[&amp;quot;metre&amp;quot;,1,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Validate Succeeds

PROJ.4 : &#39;+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs &#39;

OGC WKT :
PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,
    GEOGCS[&amp;quot;ETRS89&amp;quot;,
        DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,
            SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,
                AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],
            TOWGS84[0,0,0,0,0,0,0],
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],
        PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],
        UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],
        AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],
    PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],
    PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],
    PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],
    PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],
    PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],
    UNIT[&amp;quot;metre&amp;quot;,1,
        AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],
    AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;where-is-the-datacube-definition-stored&#34;&gt;&lt;strong&gt;Where is the datacube definition stored?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At the top level of the generated datacube, a text file will be generated (&lt;code&gt;datacube-definition.prj&lt;/code&gt;). This file is key for all
&lt;strong&gt;FORCE&lt;/strong&gt; higher-level functionality. Each higher-level module will save a copy of this file in the corresponding output directory. If this file is not present, the tools will fail. Therefore, &lt;strong&gt;do not modify, move, or delete this file&lt;/strong&gt;. This file contains the datacube definition as defined above.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;projection in WKT&lt;/li&gt;
&lt;li&gt;grid origin, longitude&lt;/li&gt;
&lt;li&gt;grid origin, latitude&lt;/li&gt;
&lt;li&gt;grid origin, x-coordinate in projection&lt;/li&gt;
&lt;li&gt;grid origin, y-coordinate in projection&lt;/li&gt;
&lt;li&gt;tile size in projection units&lt;/li&gt;
&lt;li&gt;block size in projection units&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/edc/level2/datacube-definition.prj
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,GEOGCS[&amp;quot;ETRS89&amp;quot;,DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],UNIT[&amp;quot;metre&amp;quot;,1,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]
-25.000000
60.000000
2456026.250000
4574919.500000
30000.000000
3000.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;In some rare circumstances, you might need to generate this file on your own. However, this only applies if - for any reason - you skip the Level 2 processing (e.g. if you only want to work with external features, or trick &lt;strong&gt;FORCE&lt;/strong&gt; into using external ARD datasets).&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-is-the-datacube-organized&#34;&gt;&lt;strong&gt;How is the datacube organized?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In practice, the tiles are directories in the file system, and each chip represents one file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2 | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;X0134_Y0095
X0134_Y0096
X0134_Y0097
X0134_Y0098
X0134_Y0099
X0135_Y0095
X0135_Y0096
X0135_Y0097
X0135_Y0098
X0135_Y0099
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X0134_Y0097/*.tif | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_VZN.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_VZN.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within the tile, &lt;strong&gt;FORCE&lt;/strong&gt; semantically groups files into datasets if they have the same sensor and date (e.g. multiple products like Bottom-of-Atmosphere reflectance &lt;code&gt;BOA&lt;/code&gt; and Quality Assurance Information &lt;code&gt;QAI&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_*.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_VZN.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note that chips in different tiles have the same filename, thus they can easily be mosaicked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X*/20181225_LEVEL2_SEN2A_BOA.tif | wc -l
ls /data/Dagobah/edc/level2/X*/20181225_LEVEL2_SEN2A_BOA.tif | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;49
/data/Dagobah/edc/level2/X0133_Y0100/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0096/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0098/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0099/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0095/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0096/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0098/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0099/20181225_LEVEL2_SEN2A_BOA.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;i-processed-quite-some-data-there-are-many-many-tiles-how-do-i-find-a-poi&#34;&gt;&lt;strong&gt;I processed quite some data. There are many, many tiles. How do I find a POI?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Given any coordinate \((\lambda,\phi)\), the computation of the corresponding tile is pretty straightforward.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Convert the coordinate \((\lambda,\phi)\) to the projected coordinate \((X,Y)\)&lt;/li&gt;
&lt;li&gt;Given the tile size \(t_s\) and the grid origin in projected coordinates \((X_O,Y_O)\), the tile ID can be computed as \(Tile_X = floor((X-X_O)/t_s)\) and \(Tile_Y = floor((Y_O-Y)/t_s)\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With some more math, you can also compute the exact pixel.&lt;/p&gt;
&lt;p&gt;However, there is also a &lt;strong&gt;FORCE&lt;/strong&gt; program that relieves you from doing this on your own:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tile-finder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-tile-finder datacube lon lat res
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tile-finder /data/Dagobah/edc/level2 13.404194 52.502889 10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Point { LON/LAT (13.40,52.50) | X/Y (4552071.50,3271363.25) }
  is in tile X0069_Y0043 at pixel 2604/1355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another useful &lt;strong&gt;FORCE&lt;/strong&gt; program can generate a vector file (shapefile or kml) for convenient display of the tiles.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tabulate-grid
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-tabulate-grid datacube bottom top left right format
             format: shp or kml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tabulate-grid /data/Dagobah/edc/level2 35 60 0 20 kml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/datacube-grid.kml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The grid can easily be loaded in GoogleEarth or any GIS. The attribute table contains the tile ID.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-google-grid.jpg&#34; data-caption=&#34;Exported grid loaded in Google Earth&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-google-grid.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Exported grid loaded in Google Earth
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;how-to-visualize-data-for-a-large-extent-more-conveniently&#34;&gt;&lt;strong&gt;How to visualize data for a large extent more conveniently?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Whenever you use a FORCE routine, cubed data will be generated. It is a bit cumbersome to display such data for a large extent without some further treatment. The following recipe can be used for any cubed &lt;strong&gt;FORCE&lt;/strong&gt; data - irrespective of processing level.&lt;/p&gt;
&lt;p&gt;Lucky us, the &lt;a href=&#34;https://gdal.org/drivers/raster/vrt.html&#34;&gt;GDAL virtual format&lt;/a&gt; represents an ideal concept for this. With VRTs, mosaicks of cubed data can be generated without physically copying the data. The VRT is basically a text file in xml-Format, which both holds (relative) links to the original data and the rules to assemble the mosaic on-the-fly.
&lt;strong&gt;FORCE&lt;/strong&gt; comes with a tool to generate such mosaics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-mosaic tiled-archive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/Dagobah/edc/level2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;force-mosaic searches for image files in the datacube, and mosaics all files with the same basename. The mosaics are stored in the &lt;code&gt;mosaic&lt;/code&gt; subdirectory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic | head
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;19840328_LEVEL2_LND05_BOA.vrt
19840328_LEVEL2_LND05_CLD.vrt
19840328_LEVEL2_LND05_HOT.vrt
19840328_LEVEL2_LND05_QAI.vrt
19840328_LEVEL2_LND05_VZN.vrt
19840409_LEVEL2_LND05_BOA.vrt
19840409_LEVEL2_LND05_CLD.vrt
19840409_LEVEL2_LND05_HOT.vrt
19840409_LEVEL2_LND05_QAI.vrt
19840409_LEVEL2_LND05_VZN.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To speed up visualization, pyramids might be generated for the VRT files. This significantly increases loading and response times for visualization. However, pyramid layers are basically copies of the original data at reduced resolution, and as such, they consume some disc space. Consider from case to case whether fast display merits the excess disc usage. &lt;strong&gt;FORCE&lt;/strong&gt; comes with a tool to generate pyramids:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-pyramid
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-pyramid file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pyramids for one file can be generated with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-pyramid /data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
computing pyramids for 19840828_LEVEL2_LND05_BOA.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Practically, a DEFLATE compressed overview image will be stored next to the VRT:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt.ovr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pyramids for all VRT mosaics can be parallely generated with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic/*.vrt | parallel force-pyramid {}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any modern software based on GDAL (e.g. QGIS) is able to display VRTs, and can also handle the attached pyramid layers. Mosaicking is done on-the-fly, data outside of the display extent are not loaded.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-mosaic.jpg&#34; data-caption=&#34;VRT mosaick loaded in QGIS&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-mosaic.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    VRT mosaick loaded in QGIS
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Quality Bits a.k.a. Cloud Masks etc.</title>
      <link>https://davidfrantz.github.io/tutorials/force-qai/qai/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-qai/qai/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will explain what quality bits are, how quality bits are implemented in &lt;strong&gt;FORCE&lt;/strong&gt;, how to visualize them, and how to deal with them in Higher Level Processing.&lt;/p&gt;
&lt;h2 id=&#34;what-are-quality-bits&#34;&gt;&lt;strong&gt;What are quality bits?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;FORCE L2PS&lt;/strong&gt; provides a description of the quality of each pixel in the form of quality bits. This bit-packed information allows users to apply per pixel filters to all Level 2 products. The bits represent combinations of surface, atmospheric, and processing-related conditions that can affect the overall usefulness of a given pixel for a particular application. The success of any follow-up analysis depends on the rigorous usage of these information!
A good explanation of quality bits is given by the &lt;a href=&#34;https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1-level-1-quality-assessment-band?qt-science_support_page_related_con=0#qt-science_support_page_related_con&#34;&gt;USGS&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The bit-packed information in the QA bands is a translation of binary strings. For example, the integer value “1” translates to the binary value “0001.” The binary value “0001” has 4 bits, written right to left as bits 0 (“1”), 1 (“0”), 2 (“0”), and 3 (“0”). Each of the bits 0-3 represents a condition that can affect the calculation of a physical value. [&amp;hellip;] If the condition is true, the bit is set to “1,” or “0” if false.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;sounds-complicated-why-not-use-a-scene-classification&#34;&gt;&lt;strong&gt;Sounds complicated… Why not use a scene classification?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Although interpretation of quality bits is not immediate to humans, they do possess quite some advantages. As opposed to a scene classification, quality bits allow the flagging of multiple conditions, e.g. ice clouds, cloud shadows on top of clouds or snow, high aerosol load and cloud, etc. If a 16bit Integer is used for storing the quality bits, up to 16 different conditions can co-exist in any possible combination. In a scene classification, only one condition can be stored, and the algorithm developer needs to make assumptions on the priority of the conditions; however these may differ from application to application. Quality bits allow to store all these information in a single  image. From a technical perspective, quality bits save disc space, and reduce the I/O load for follow-up analyses.&lt;/p&gt;
&lt;h2 id=&#34;quality-bits-in-force&#34;&gt;&lt;strong&gt;Quality bits in FORCE&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;strong&gt;FORCE&lt;/strong&gt;, the quality bits are found in the Quality Assurance Information (&lt;code&gt;QAI&lt;/code&gt;) product, which is an integral part of each Level 2 dataset, and is alway present next to the reflectance images (&lt;code&gt;BOA&lt;/code&gt; or &lt;code&gt;TOA&lt;/code&gt;).
When generating Best Available Pixel (&lt;code&gt;BAP&lt;/code&gt;) composites (Level 3), the bit flags of the selected observation are stored in the first band of the composite information (&lt;code&gt;INF&lt;/code&gt;) product.
Currently &lt;strong&gt;FORCE&lt;/strong&gt; implements a 16bit QAI layer with 12 quality bits, some of them as double-bit words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Bit No.&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Parameter name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Bit comb.&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Integer&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;State&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Valid data&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;valid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1–2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cloud state&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;clear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;01&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;less confident cloud (i.e., buffered cloud 300 m)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;confident, opaque cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cirrus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cloud shadow flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Snow flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Water flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;6–7&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Aerosol state&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;estimated (best quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;01&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;interpolated (mid quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;high (aerosol optical depth &amp;gt; 0.6, use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fill (global fallback, low quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;8&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Subzero flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes (use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;9&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Saturation flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes (use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;High sun zenith flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes (sun elevation &amp;lt; 15°, use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;11–12&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Illumination state&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;00&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;good (incidence angle &amp;lt; 55°, best quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;01&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;medium (incidence angle 55°–80°, good quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;poor (incidence angle &amp;gt; 80°, low quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;11&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;shadow (incidence angle &amp;gt; 90°, no top. correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;13&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Slope flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;no (cosine correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yes (enhanced C-correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;14&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Water vapor flag&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;measured (best quality, only Sentinel-2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fill (scene average, only Sentinel-2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;15&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Empty&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;0&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Nodata values are values where nothing was observed, where auxiliary data was not given (e.g. nodata in DEM), or
where data is substantially corrupt (e.g. impulse noise, or when the surface reflectance estimate is &amp;gt; 2.0 or &amp;lt; -1.0)&lt;/li&gt;
&lt;li&gt;Clouds are given in three categories, i.e. opaque clouds (confident cloud), buffered clouds (300m; less confident cloud), and cirrus clouds.&lt;/li&gt;
&lt;li&gt;Cloud shadows are detected on the basis of the cloud layer. If a cloud is missed, the cloud shadow is missed, too. If a false
positive cloud is detected, false positive cloud shadows follow.&lt;/li&gt;
&lt;li&gt;Aerosol Optical Depth is estimated for fairly coarse grid cells. If there is no valid AOD estimation in any cell, values are
interpolated. If there is no valid AOD estimation for the complete image, a fill value is assigned (AOD is guessed). If AOD @550nm is higher than 0.6, it is flagged as high aerosol; this is not necessarily critical, but should be used with caution (see subzero flag).&lt;/li&gt;
&lt;li&gt;If the surface reflectance estimate in any band is &amp;lt; 0, the subzero flag is set. This can point to overestimation of AOD.&lt;/li&gt;
&lt;li&gt;If DNs were saturated, or if the surface reflectance estimate in any band is &amp;gt; 1, the saturation flag is set.&lt;/li&gt;
&lt;li&gt;If sun elevation is smaller than 15°, the high sun zenith flag is set. Use this data with caution, radiative transfer computations might be out of specification.&lt;/li&gt;
&lt;li&gt;The illumination state is related to the quality of the topographic correction. If the incidence angle is smaller than 55°, quality is best. If the incidence angle is larger than 80°, the quality of the topographic correction is low, and data artefacts are possible. If the area is not illuminated at all, no topographic correction is done (values are the same as without topographic correction).&lt;/li&gt;
&lt;li&gt;The slope flag indicates whether a simple cosine correction (slope ≤ 2°) was used for topographic correction, or if the enhanced C-correction was used (slope &amp;gt; 2°).&lt;/li&gt;
&lt;li&gt;The water vapor flag indicates whether water vapor was estimated, or if the scene average was used to fill. Water vapor is not estimated over water and cloud shadow pixels. This flag only applies to Sentinel-2 images.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;visualization&#34;&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visualizing the raw QAI image is pretty meaningless. Don’t be surprised that the integers do not resemble any of the patterns you would expect (e.g. cloud distribution).&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-boa.jpg&#34; data-caption=&#34;Sentinel-2B image over Berlin, 01.07.2019; left: RGB image; right: quality bits&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-boa.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sentinel-2B image over Berlin, 01.07.2019; left: RGB image; right: quality bits
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;quicklooks&#34;&gt;&lt;strong&gt;Quicklooks&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since v. 3.0, &lt;strong&gt;FORCE L2PS&lt;/strong&gt; can output quicklook images for each Level 2 dataset (&lt;code&gt;OVV&lt;/code&gt; = overview product). These thumbnails serve as first impression of image quality. Some of the quality conditions are superimposed on the RGB images. Opaque clouds are shown in pink, cirrus clouds in red, cloud shadows in cyan, snow in yellow, saturated pixels in orange, and sub-zero reflectance values in a greenish tone. The overview for the QAI image from above is shown here:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-ovv.jpg&#34; data-caption=&#34;Quicklook image generated by FORCE L2PS; pink: opaque clouds; cyan: cloud shadows&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-ovv.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Quicklook image generated by &lt;strong&gt;FORCE L2PS&lt;/strong&gt;; pink: opaque clouds; cyan: cloud shadows
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;inflate-quality-bits&#34;&gt;&lt;strong&gt;Inflate quality bits&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A full deciphering of all quality bits to individual quality masks can be generated with &lt;strong&gt;FORCE&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-qai-inflate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-qai-inflate QAI dir format
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-qai-inflate /data/level2/X0069_Y0043/20190701_LEVEL2_SEN2B_QAI.tif ~/temp GTiff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This program generates a 12-band image, where each of the flags (see table above) is written to a separate band. However, force-qai-inflate was not designed to generate inflated masks for each and every Level 2 product in a routine manner due to the computational and disc-space related overhead. We strongly recommend to make use of  the bits directly (see remaining part of the tutorial).&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-cld.jpg&#34; data-caption=&#34;Quality bits; left: cloud state; right: cloud shadow flag&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-cld.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Quality bits; left: cloud state; right: cloud shadow flag
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;quality-bit-rendering-in-qgis&#34;&gt;&lt;strong&gt;Quality bit rendering in QGIS&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;There is a nice QGIS plugin from my colleague &lt;a href=&#34;https://www.geographie.hu-berlin.de/en/professorships/eol/people/labmembers/benjamin_jakimow&#34;&gt;Benjamin Jakimow&lt;/a&gt;, which can visualize quality bits in QGIS &lt;em&gt;on the fly&lt;/em&gt;. Quality bit inflating is not necessary anymore! The &lt;a href=&#34;http://plugins.qgis.org/plugins/BitFlagRenderer/&#34;&gt;Bit Flag Renderer plugin&lt;/a&gt; provides a new renderer for QGIS, with which any quality bit product can flexibly be visualized. The plugin includes predefined bit visualization rules for the &lt;strong&gt;FORCE&lt;/strong&gt; QAI bits. The default visualization matches the information and colors from the quicklook images described above):&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-bfr.jpg&#34; data-caption=&#34;Bit Flag Renderer in QGIS displaying a quality bit layer on-the-fly with the pre-defined FORCE settings&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-bfr.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bit Flag Renderer in QGIS displaying a quality bit layer on-the-fly with the pre-defined FORCE settings
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;quality-masking-in-higher-level-processing&#34;&gt;&lt;strong&gt;Quality masking in Higher Level Processing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For follow-up processing and analyses, the usage of the QAI information is key, e.g. to exclude clouds. In all &lt;strong&gt;FORCE Higher Level routines&lt;/strong&gt;, quality masking is done on the fly, and the user has full control about what condition(s) to filter. The parameter &lt;code&gt;SCREEN_QAI&lt;/code&gt; provides a simple mechanism to mask out any combination of conditions using any of the following keywords: &lt;em&gt;NODATA, CLOUD_OPAQUE, CLOUD_BUFFER, CLOUD_CIRRUS, CLOUD_SHADOW, SNOW, WATER, AOD_FILL, AOD_HIGH, AOD_INT, SUBZERO, SATURATION, SUN_LOW, ILLUMIN_NONE, ILLUMIN_POOR, ILLUMIN_LOW, SLOPED, WVP_NONE&lt;/em&gt;. The default parametrization is to filter out nodata, clouds, cloud shadows, snow, saturated or subzero reflectance:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SCREEN_QAI = NODATA CLOUD_OPAQUE CLOUD_BUFFER CLOUD_CIRRUS CLOUD_SHADOW SNOW SUBZERO SATURATION&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Following images illustrate the effect of quality filtering on an average reflectance image generated by using all available observations over a 3 month period (using Spectral Temporal Metrics in the &lt;strong&gt;Time Series Analysis module&lt;/strong&gt;). The left image was produced by filtering nodata values only, the right image was produced using the default quality screening.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-avg.jpg&#34; data-caption=&#34;Average reflectance over three month; left: not using quality bits; right with quality bits&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-avg.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Average reflectance over three month; left: &lt;strong&gt;not using&lt;/strong&gt; quality bits; right &lt;strong&gt;with&lt;/strong&gt; quality bits
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Water Vapor Database</title>
      <link>https://davidfrantz.github.io/tutorials/force-wvdb/wvdb/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-wvdb/wvdb/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will show how to prepare the Water Vapor Database (WVDB) for the &lt;strong&gt;FORCE Level 2 Processing System (FORCE L2PS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Chart start 1&lt;/p&gt;



  


&lt;div id=&#34;chart-165248397&#34; class=&#34;chart pb-3&#34; style=&#34;max-width: 100%; margin: auto;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;/img/test.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-165248397&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();

&lt;/script&gt;
&lt;p&gt;Chart end 1&lt;/p&gt;
&lt;p&gt;Chart start 2&lt;/p&gt;



  


&lt;div id=&#34;chart-219456378&#34; class=&#34;chart pb-3&#34; style=&#34;max-width: 100%; margin: auto;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;/img/network.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-219456378&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();

&lt;/script&gt;
&lt;p&gt;Chart end 2&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;During atmospheric correction, the effect of water vapor absorption can only be corrected if we know the amount of water vapor in the atmosphere.&lt;/p&gt;
&lt;p&gt;If you are using Sentinel-2 data only, you can stop reading. Sentinel-2 is equipped with a water vapor channel, and thus, water wapor amount can be estimated from the images.&lt;/p&gt;
&lt;p&gt;Landsat, however, doesn&amp;rsquo;t have such a band. Therefore, we need to rely on external data, which needs to be precompiled into a water vapor database.&lt;/p&gt;
&lt;h2 id=&#34;water-vapor-database&#34;&gt;&lt;strong&gt;Water Vapor Database&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database holds water vapor values for the central coordinates of each WRS-2 frame. If available, day-specific values are used.&lt;/p&gt;
&lt;p&gt;The database consists of one table for each day (&lt;code&gt;WVP_YYYY-MM-DD.txt&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Earth/global/wvp/wvdb/WVP_2010-07-*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Earth/global/wvp/wvdb/WVP_2010-07-01.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-02.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-03.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-04.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-05.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-06.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-07.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-08.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-09.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-10.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-11.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-12.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-13.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-14.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-15.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-16.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-17.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-18.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-19.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-20.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-21.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-22.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-23.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-24.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-25.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-27.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-28.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-29.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-30.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-31.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each file includes one value per coordinate. In the example below, there are 13281 coordinates in each file (global land coverage). The coordinate, which is closest to the center of the Landsat image is selected, and the atmospheric correction uses this value to account for gaseous absorption.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.3934 80.7603 1.170018 MOD
-22.8654 80.0056 9999.000000 TBD
-29.2236 79.1137 9999.000000 TBD
-34.5930 78.1151 0.614454 MOD
-39.1269 77.0343 0.448552 MOD
-42.9718 75.8898 0.260607 MOD
-46.2552 74.6958 0.282855 MYD
-49.0816 73.4629 0.337015 MOD
-51.5357 72.1989 9999.000000 TBD
-53.6847 70.9100 9999.000000 TBD
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;climatology&#34;&gt;&lt;strong&gt;Climatology&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If day-specific values are not available (no table is existing, or there is a fill value), a monthly long-term climatology is used instead. The climatology consists of one table for each month (&lt;code&gt;WVP_0000-MM-00.txt&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Earth/global/wvp/wvdb/WVP_0000*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Earth/global/wvp/wvdb/WVP_0000-01-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-02-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-03-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-04-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-05-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-06-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-08-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-09-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-10-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-11-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-12-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, each file includes one value per coordinate. The file holds the long-term average, long-term standard deviation, and the number of measurements used to compute these statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.3934 80.7603 1.177704 0.364894 300
-22.8654 80.0056 1.079682 0.328948 311
-29.2236 79.1137 0.695211 0.234917 383
-34.5930 78.1151 0.549352 0.256754 445
-39.1269 77.0343 0.472883 0.224957 480
-42.9718 75.8898 0.410826 0.211346 476
-46.2552 74.6958 0.384219 0.145523 457
-49.0816 73.4629 0.415261 0.170940 456
-51.5357 72.1989 0.515858 0.223122 422
-53.6847 70.9100 0.546611 0.273735 276
&lt;/code&gt;&lt;/pre&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/wvdb.gif&#34; data-caption=&#34;Global animation of the climatology (monthly average)&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/wvdb.gif&#34; alt=&#34;&#34; width=&#34;750&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Global animation of the climatology (monthly average)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;uncertainty-of-the-climatology&#34;&gt;&lt;strong&gt;Uncertainty of the climatology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The uncertainty of using the climatology was assessed in this paper:
Frantz, D., Stellmes, M., &amp;amp; Hostert, P. (2019). A Global MODIS Water Vapor Database for the Operational Atmospheric Correction of Historic and Recent Landsat Imagery. Remote Sensing, 11, 257. &lt;a href=&#34;https://doi.org/10.3390/rs11030257&#34;&gt;https://doi.org/10.3390/rs11030257&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;prepare-the-wvdb&#34;&gt;&lt;strong&gt;Prepare the WVDB&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We generally use a WVDB generated from MODIS water vapor products (&lt;a href=&#34;https://modis.gsfc.nasa.gov/data/dataprod/mod05.php&#34;&gt;MOD05 and MYD05&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;download-the-ready-to-go-global-wvdb&#34;&gt;&lt;strong&gt;Download the ready-to-go global WVDB&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;You should start by downloading the pre-compiled WVDB with global coverage from &lt;a href=&#34;doi.pangaea.de/10.1594/PANGAEA.893109&#34;&gt;here&lt;/a&gt;. This saves you a lot of processing. This freely available dataset was generated with the &lt;strong&gt;FORCE WVDB&lt;/strong&gt; component, and is comprised of daily global water vapor data for February 2000 to July 2018 for each land-intersecting WRS-2 scene (13281 coordinates), as well as a monthly climatology that can be used if no daily value is available.&lt;/p&gt;
&lt;h3 id=&#34;generate-the-wvdb-on-your-own&#34;&gt;&lt;strong&gt;Generate the WVDB on your own&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We try to update this dataset in regular intervals. However, if you are in need of more up-to-date data, you can use the &lt;strong&gt;FORCE WVDB&lt;/strong&gt; component to generate/update these tables on your own.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note that you need access to the LAADS DAAC before using this tool (see last section on this page).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FORCE WVDB&lt;/strong&gt; needs a table with input coordinates (center coordinates of WRS-2 frames). The &lt;a href=&#34;doi.pangaea.de/10.1594/PANGAEA.893109&#34;&gt;pre-compiled dataset&lt;/a&gt; includes such a table. If you are not interested in global coverage, you can subset this file. The file should contain two columns separated by white space, and no header. The first column should give the longitude (X), the second column the latitude (Y) with coordinates in decimal degree (negative values for West/South). Any other column is ignored (in the example below, the WRS-2 Path/Row is in the third column).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.39340494140 80.76026666750 013001
-22.86543244600 80.00558606640 013002
-29.22356065160 79.11366800820 013003
-34.59295680040 78.11513723200 013004
-39.12687451150 77.03430642440 013005
-42.97184515330 75.88984431700 013006
-46.25519224080 74.69581438230 013007
-49.08160498390 73.46286239410 013008
-51.53569902300 72.19888348300 013009
-53.68466715610 70.91003752470 013010
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;FORCE WVDB&lt;/strong&gt; downloads each Terra/Aqua granule (collection 6.1) that intersects with any of these coordinates. The files are downloaded from the Level1 and Atmosphere Archive and Distribution System (&lt;a href=&#34;ladsweb.modaps.eosdis.nasa.gov&#34;&gt;LAADS&lt;/a&gt;) at NASA’s Goddard Space Flight Center. Note that any permanent or temporary change/shutdown/decommissioning on LAADS’ or MODIS’ end may result in the nonfunctioning of &lt;strong&gt;FORCE WVDB&lt;/strong&gt;&amp;hellip; Also note, that they perform a weekly maintenance, during which their servers are not accessable.&lt;/p&gt;
&lt;p&gt;As with any other FORCE program, you can display short usage instructions by executing the program without any parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-lut-modis coords dir-wvp dir-geometa dir-eoshdf
           [start-year start-month start-day
            end-year   end-month   end-day]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A coordinate file needs to be given as 1st argument.&lt;/p&gt;
&lt;p&gt;The MODIS data are downloaded to dir-eoshdf (this directory must exist). MODIS data that are already in dir-eoshdf are not downloaded again. &lt;em&gt;If the tool crashes because a dataset is corrupt, it is necessary to manually delete this file and run the tool again. Unfortunately, this happens from time to time due to incomplete downloads or if LAADS is unresponsive. The program attempts to re-download a corrupt file up to 10 times, but this error can occur nonetheless.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MOD05/MYD05 data are swath products, and MOD03/MYD03 geometa tables are necessary to relate coordinates to MODIS granules. The geometa tables are downloaded to dir-geometa (this directory must exist). Tables that are already in dir-geometa are not downloaded again. &lt;em&gt;If the tool crashes because a table is invalid, it is necessary to manually delete this file and run the tool again. Unfortunately, this happens from time to time due to incomplete downloads or if LAADS is unresponsive. The program attempts to re-download a corrupt file up to 10 times, but this error can occur nonetheless.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The final water vapor tables are saved in dir-wvp (this directory must exist). Tables that are already in dir-wvp are not processed again (i.e. no download of geometa tables and hdf files).&lt;/p&gt;
&lt;p&gt;The start and end arguments are optional and may be used for parallelization. If they are not given, &lt;strong&gt;FORCE WVDB&lt;/strong&gt; will download the entire time series of all coordinates provided (this can be a lot!).&lt;/p&gt;
&lt;p&gt;This directory is the directory, to which DIR_WVPLUT in the &lt;strong&gt;FORCE L2PS&lt;/strong&gt; parameter file should refer.
&lt;code&gt;DIR_WVPLUT = /data/Earth/global/wvp/wvdb&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you have finished compiling the WVDB, you may delete the MODIS *.hdf files.&lt;/p&gt;
&lt;p&gt;Download the entire data record (in one process - this is slow):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Download one week:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf 2010 07 01 2010 07 07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use GNU parallel to download an entire month in 31 parallel processes. This works by creating a list 1..31, which is distributed to 31 jobs. Each job calls &lt;strong&gt;FORCE WVDB&lt;/strong&gt; for one specific day in July 2010. The curly braces are replaced with the list value given to each process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;seq -w 1 31 | parallel -j31 force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf 2010 07 {} 2010 07 {}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;get-access-to-the-laads-daac-edit-13022020&#34;&gt;&lt;strong&gt;Get access to the LAADS DAAC&lt;/strong&gt; &lt;em&gt;(edit 13.02.2020)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;You need authentification to download data from the LAADS DAAC. This works by requesting an App Key from &lt;a href=&#34;https://ladsweb.modaps.eosdis.nasa.gov/tools-and-services/data-download-scripts/#requesting&#34;&gt;NASA Earthdata&lt;/a&gt;. You can make this key available to &lt;strong&gt;FORCE&lt;/strong&gt; by putting the character string in a file &lt;code&gt;.laads&lt;/code&gt; in your home directory. With this, you should be able to download data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
