<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>FORCE | DAVID FRANTZ</title>
    <link>https://davidfrantz.github.io/tags/force/</link>
      <atom:link href="https://davidfrantz.github.io/tags/force/index.xml" rel="self" type="application/rss+xml" />
    <description>FORCE</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019-2020 David Frantz</copyright><lastBuildDate>Wed, 09 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://davidfrantz.github.io/img/icon-192.png</url>
      <title>FORCE</title>
      <link>https://davidfrantz.github.io/tags/force/</link>
    </image>
    
    <item>
      <title>FORCE Tutorial: Spectral Temporal Metrics</title>
      <link>https://davidfrantz.github.io/tutorials/force-stm/stm/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-stm/stm/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.5&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains what Spectral Temporal Metrics are, and how to generate them using the Time Series Analysis (TSA) submodule of the &lt;strong&gt;FORCE Higher Level Processing system (HLPS)&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-are-spectral-temporal-metrics&#34;&gt;&lt;strong&gt;What are Spectral Temporal Metrics?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;There are also other names around, which describe the same concept.
However, regardless of how you name it, Spectral Temporal Metrics (or simply STMs) are band-wise descriptive statistics, which summarize reflectance (or an index derived thereof) within a defined time period.
This can be the annual mean, standard deviation or median.
Before calculating the statistics, the data is quality filtered, e.g. clouds are removed.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-stm-example.png&#34; data-caption=&#34;STM concept © Stefan Ernst&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-stm-example.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    STM concept © Stefan Ernst
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;It is important to note that STMs are statistical aggregations - and not composites as in &lt;em&gt;“we select the full spectrum for the optimal observation”&lt;/em&gt;.
Thus, some care should be taken when dealing with these kinds of metrics: an STM spectrum cannot be interpreted in the classic remote-sensing-textbook sense.
As an example, computing the NDVI from the 25% quantile of red and near infrared reflectance may yield unexpected results as they are not originating from the same acquisition date…
Funny things may happen when you do.
To emphasize this distinction, this is why we generally do not term them composites - but STMs (&lt;em&gt;this is debatable, I know&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Anyway, STMs are powerful, yet simple (&lt;em&gt;simple is good!&lt;/em&gt;) metrics that are often used as features for Machine Learning algorithms, both for classification and regression problems.
Due to their robustness (as compared to more advanced metrics) and their spatial completeness, they simplify the extrapolation/mapping of a response variable (qualitative or quantitative, e.g. classification labels or forest biomass) across very large areas.&lt;/p&gt;
&lt;h2 id=&#34;parameterfile&#34;&gt;&lt;strong&gt;Parameterfile&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;STMs are available within the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/tsa/index.html&#34;&gt;Time Series Analysis (TSA)&lt;/a&gt; submodule of the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/index.html&#34;&gt;FORCE Higher Level Processing system (HLPS)&lt;/a&gt;.
You can use &lt;code&gt;force-parameter&lt;/code&gt; to generate an empty parameterfile.&lt;/p&gt;
&lt;p&gt;To familiarize yourself with the TSA submodule, I suggest taking a detour to the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-tsi/tsi/&#34;&gt;interpolation tutorial&lt;/a&gt; before advancing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Temporal extent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The temporal extent defines the time period, which is used for aggregating the Level 2 observations into STMs.
Broader windows generally result in cleaner metrics, although at the expense of seasonal information.
It is hard to give a suggestion here, as this is a complex decision, which depends on sensors used, data availability, environmental conditions, cloud occurence, phenology, what you want to do with the data etc.
To demonstrate, we are generating annual STMs for entire 2018:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TIME_RANGE = 2018-01-01 2018-12-31&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sensors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;FORCE HLPS allows you to easily combine data from different sensors - provided that we only use mutually available bands.
For this tutorial, we are using data from Landsat 8, Sentinel-2A and Sentinel-2B.
We are setting the output resolution to 30m.
For Landsat, this is the native resolution.
For the 10m Sentinel-2, we degrade the resolution using an approximated Point Spread Function (Gaussian lowpass with FWHM = analysis resolution), which approximates the acquisition of data at lower spatial resolution.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SENSORS = LND08 SEN2A SEN2B
RESOLUTION = 30
REDUCE_PSF = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Bands / Indices&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will produce STMs for some spectral bands, as well as some indices:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;INDEX = RED NIR SWIR1 NDVI NDBI MNDWI&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Outlier detection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you see cloud remnants in your STMs, you might want to experiment with the outlier detection option.
For now, lets disable it with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ABOVE_NOISE = 0
BELOW_NOISE = 0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before temporally aggregating the L2 observations, we can interpolate the time series. Try this out! But for now, let’s go without:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;INTERPOLATE = NONE&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;STMs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, we define the statistics for producing the STMs.
You can specify a list with all statistics at once.
Currently available are&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ID&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AVG&lt;/td&gt;
&lt;td&gt;Average&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;STD&lt;/td&gt;
&lt;td&gt;Standard deviation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MIN&lt;/td&gt;
&lt;td&gt;Minimum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MAX&lt;/td&gt;
&lt;td&gt;Maximum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RNG&lt;/td&gt;
&lt;td&gt;Range&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QXX&lt;/td&gt;
&lt;td&gt;Quantiles, replace XX with any 2-digit number, e.g. Q50 for the median. Multiple quantiles can be given&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IQR&lt;/td&gt;
&lt;td&gt;Inter-quartile range&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SKW&lt;/td&gt;
&lt;td&gt;Skewness&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;KRT&lt;/td&gt;
&lt;td&gt;Kurtosis&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NUM&lt;/td&gt;
&lt;td&gt;Number of observations (after outlier detection and interpolation)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Let’s go with these metrics for now:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;STM = Q10 Q25 Q50 Q75 Q90 AVG STD
OUTPUT_STM = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Explode Output?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By default, HLPS will produce multi-band files for each spectral band/index, i.e. you will get one file for each index, which will have as many bands as there are STMs.
If you rather prefer single-band images, i.e. one file for each index and each STM, use&lt;/p&gt;
&lt;p&gt;&lt;code&gt;OUTPUT_EXPLODE = TRUE&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other parameters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The other parameters are not relevant for generating STMs. However, please note that you can generate STMs AND use the other options at the same time, e.g. Trend Analysis, Land Surface Phenology, etc. This saves time as data is only read once.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Processing is straightforward. Simply use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-higher-level /data/europe/stm/stm.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;number of processing units: 280
 (active tiles: 28, chunks per tile: 10)
________________________________________
Progress:                        100.00%
Time for I/C/O:           087%/008%/004%
ETA:             00y 00m 00d 00h 00m 00s

________________________________________
Real time:       00y 00m 00d 00h 19m 05s
Virtual time:    00y 00m 00d 00h 21m 35s
Saved time:      00y 00m 00d 00h 02m 30s

________________________________________
Virtual I-time:  00y 00m 00d 00h 18m 53s
Virtual C-time:  00y 00m 00d 00h 01m 47s
Virtual O-time:  00y 00m 00d 00h 00m 55s

________________________________________
I-bound time:    00y 00m 00d 00h 17m 10s
C-bound time:    00y 00m 00d 00h 00m 07s
O-bound time:    00y 00m 00d 00h 00m 03s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this, we generate a mosaic.
With &lt;code&gt;OUTPUT_EXPLODE = TRUE&lt;/code&gt;, you get one image for each requested index and statistical aggregation, i.e. 42 images in our case:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/europe/stm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mosaicking 42 products:
1 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_AVG.tif
2 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q10.tif
3 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q25.tif
4 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q50.tif
...
40 2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_Q75.tif
41 2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_Q90.tif
42 2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_STD.tif

mosaicking 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_AVG.tif
26 chips found.

mosaicking 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q25.tif
26 chips found.

...

mosaicking 2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_AVG.tif
26 chips found.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualization&#34;&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visualizing an RGB color composite in QGIS, wherein the 3 bands come from different physical files, does not work out of the box..
Thus, we need to put the required bands into one file. Luckily, a virtual data format suffices.
This example here stacks the 50% quantiles of the reflectance bands, as well as the 90% quantiles of the indices.
For fast visualization, we are computing pyramids.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /data/europe/stm/mosaic
force-stack *RED_STM_Q50.vrt *NIR_STM_Q50.vrt *SW1_STM_Q50.vrt stack-bands-STM_Q50.vrt
force-stack *NDB_STM_Q90.vrt *NDV_STM_Q90.vrt *MNW_STM_Q90.vrt stack-indices-STM_Q90.vrt
ls *.vrt | parallel force-pyramid {}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;file 1:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_RED_STM_Q50.vrt
  9000 4000 1
file 2:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_NIR_STM_Q50.vrt
  9000 4000 1
file 3:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_Q50.vrt
  9000 4000 1

Same number of bands detected. Stacking by band.

Band 0001: 2018-2018_001-365_HL_TSA_LNDLG_RED_STM_Q50.vrt band 1
Band 0002: 2018-2018_001-365_HL_TSA_LNDLG_NIR_STM_Q50.vrt band 1
Band 0003: 2018-2018_001-365_HL_TSA_LNDLG_SW1_STM_Q50.vrt band 1

file 1:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_NDB_STM_Q90.vrt
  9000 4000 1
file 2:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_NDV_STM_Q90.vrt
  9000 4000 1
file 3:
  /data/europe/stm/mosaic
  2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q90.vrt
  9000 4000 1

Same number of bands detected. Stacking by band.

Band 0001: 2018-2018_001-365_HL_TSA_LNDLG_NDB_STM_Q90.vrt band 1
Band 0002: 2018-2018_001-365_HL_TSA_LNDLG_NDV_STM_Q90.vrt band 1
Band 0003: 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q90.vrt band 1

computing pyramids for 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q10.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
computing pyramids for 2018-2018_001-365_HL_TSA_LNDLG_MNW_STM_Q25.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
...
computing pyramids for stack-bands-STM_Q50.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
computing pyramids for stack-indices-STM_Q90.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-stm-qgis.jpg&#34; data-caption=&#34;RGB composite of STMs - Top: Q50 reflectance - Bottom: Q90 Indices&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-stm-qgis.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    RGB composite of STMs - Top: Q50 reflectance - Bottom: Q90 Indices
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Interpolation</title>
      <link>https://davidfrantz.github.io/tutorials/force-tsi/tsi/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-tsi/tsi/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.5&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial introduces the Time Series Analysis (TSA) submodule of the &lt;strong&gt;FORCE Higher Level Processing system (HLPS)&lt;/strong&gt;, shows how to interpolate time series, and how to visualize and animate them via QGIS plugins.&lt;/p&gt;
&lt;h2 id=&#34;why-interpolation&#34;&gt;&lt;strong&gt;Why interpolation?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Despite regular observations, EO time series are generally not equidistant.
Nominal equidistance is only given for areas that are not covered by orbital overlaps (16 days revisit at nadir for a single Landsat).
For areas in lateral overlaps, we have alternating revisits of 1 and 7 days.
If we are getting closer to the poles, we even get triple, quadruple etc. overlaps, which complicates this alternating pattern.
The same applies if we consider multiple sensors, e.g. two Landsat and two Sentinel-2 satellites.
And of course, nominal revisit != actual revisit.
We cannot see through clouds (unless we are talking radar).
Probably, you want to remove snow observations.
There might have been some sensor or ground segment outages, or the data couldn&#39;t have been processed to our quality requirements (e.g. tier 2 data).
The sensor might not have looked on Earth as the satellite took a detour to avoid space debris etc. etc..&lt;/p&gt;
&lt;p&gt;Interpolation might simply be used to close these gaps.
It smoothes the time series, and thus effectively reduces noise.
Equidistance is established, which might be needed for following processing steps.&lt;/p&gt;
&lt;p&gt;Time Series interpolation is a basic option within the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/tsa/index.html&#34;&gt;Time Series Analysis (TSA)&lt;/a&gt; submodule of the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/index.html&#34;&gt;FORCE Higher Level Processing system (HLPS)&lt;/a&gt;, and may be performed before using more advanced time series analysis methods:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/force-tsa-workflow.jpg&#34; data-caption=&#34;Time Series Analysis Workflow&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/force-tsa-workflow.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Time Series Analysis Workflow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;rbf-interpolation&#34;&gt;&lt;strong&gt;RBF interpolation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In FORCE, a couple of interpolation methods are implemented.
Most often, we use ensembles of Radial Basis Function (RBF) convolution filters (see this paper &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0303243416301003&#34;&gt;here&lt;/a&gt;).
As this is a convolution filter, no fitting is involved (as e.g. compared to splines or Savitzky-Golay filters), thus the method is really fast, while keeping a good balance between retaining detail and smoothing.&lt;/p&gt;
&lt;p&gt;The method is basically a moving average filter on steroids.
A kernel is moved over the time series, and the observations are weighted according to a Gaussian distribution (Gaussian lowpass).
This means that an observation in the middle of the kernel gets a higher weight.&lt;/p&gt;
&lt;p&gt;Now, data density is usually variable, which means that we have seasons with higher data availability (e.g. in the summer) - and seasons with poor data availability.
For good data situations, we would want to have a narrow Gaussian to more closely follow the actual time series.
For bad data situations, however, a narrow kernel would result in many nodata values and wouldn&#39;t smooth the time series appropriately.&lt;/p&gt;
&lt;p&gt;Therefore, we use multiple kernels of different widths.
The estimates from those kernels are eventually aggregated using a weighted average, wherein the weights correspond to the data availability within each kernel.
This gives preference to the kernel with better data coverage.&lt;/p&gt;
&lt;p&gt;This tutorial will show you how to generate an RBF-interpolated time series, and how to dynamically look at the data.
I will demonstrate this for the island of Crete (greece) in the Mediterranean Sea.
An interpolated time series looks like this:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rbf.jpg&#34; data-caption=&#34;Interpolated time series using RBF ensembles&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rbf.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Interpolated time series using RBF ensembles
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;parameterfile&#34;&gt;&lt;strong&gt;Parameterfile&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We start by generating an empty TSA parameterfile, and rename the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /data/europe/interpolation
force-parameter /data/europe/interpolation TSA 1
cd /data/europe/interpolation
mv TSA-skeleton.prm tsi.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;TSA
An empty parameter file skeleton was written to
  /data/europe/interpolation/TSA-skeleton.prm
Note that all parameters need to be given, even though some may not be used
with your specific parameterization.
You should rename the file, e.g. my-first-TSA.prm.
Parameterize according to your needs and run with
force-higher-level /data/europe/interpolation/my-first-TSA.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you prefer a more compact parameterfile without all the comments, use &lt;code&gt;0&lt;/code&gt; instead of &lt;code&gt;1&lt;/code&gt; as the last parameter.
The full set of parameters is also documented &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/tsa/param.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input / Output directories&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this tutorial, we assume that we already have an existing Level 2 ARD datapool, which contains preprocessed data for multiple years (see &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-ard/l2-ard/&#34;&gt;ARD tutorial&lt;/a&gt;).
The parent directory of this datapool goes into the &lt;code&gt;DIR_LOWER&lt;/code&gt; parameter.
The &lt;code&gt;DIR_HIGHER&lt;/code&gt; parameter names the output directory, where the interpolated time series will be generated (although technically possible, I suggest to use a different file path). Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_LOWER = /data/europe/level2
DIR_HIGHER = /data/europe/interpolation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Analysis mask&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As we are only interested in the land surface - and there is a lot of water around Crete - we use a processing mask.
Have a look at the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-masks/masks/&#34;&gt;processing mask tutorial&lt;/a&gt; for further details.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p /data/europe/masks
cp /data/europe/level2/datacube-definition.prj -t /data/europe/masks
force-cube /data/gis/admin/crete.gpkg /data/europe/masks rasterize 30
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0...10...20...30...40...50...60...70...80...90...100 - done.
0...10...20...30...40...50...60...70...80...90...100 - done.
0...10...20...30...40...50...60...70...80...90...100 - done.
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the parameterfile, use the masks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_MASK = /data/europe/masks
BASE_MASK = crete.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Output options&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For interoperability with the QGIS plugins, we will generate one output image (for each index) in GeoTiff format, which will have the interpolated dates as bands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OUTPUT_FORMAT = GTiff
OUTPUT_EXPLODE = FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All other parameters are not relevant for now.
However, please note that you can generate a lot of time series based outputs at the same time, e.g. Trend Analysis, Land Surface Phenology, etc.
This saves time as data is only read once.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spatial extent&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;FORCE HLPS requires a square processing extent, which can be set using &lt;code&gt;X_TILE_RANGE&lt;/code&gt; and &lt;code&gt;Y_TILE_RANGE&lt;/code&gt;. Then, if the extent of our region of interest is not square, we can further refine the processing extent by specifying a tile allow-list (&lt;code&gt;FILE_TILE&lt;/code&gt;). Please see the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-datacube/datacube/&#34;&gt;datacube tutorial&lt;/a&gt; for more details, and I also suggest you to have a look at the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/hl-compute.html&#34;&gt;computing model of HLPS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are different ways to obtain these values, e.g. by generating, and filtering a shapefile with tiles via &lt;code&gt;force-tabulate-grid&lt;/code&gt;. The easiest way, however, is to use &lt;code&gt;force-tile-extent&lt;/code&gt; with a vector geometry. Example for Crete, Greece:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tile-extent /data/gis/admin/crete.gpkg /data/europe/level2 /data/europe/interpolation/crete.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Suggested Processing extent:
X_TILE_RANGE = 103 111
Y_TILE_RANGE = 101 105

Processing extent is not square.
Suggest to use the tile allow-list:
FILE_TILE = /data/europe/interpolation/crete.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Block size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The block size is a parameter that you should only adjust if you are running in RAM-shortages.
&lt;em&gt;First, try the default value and don’t worry&lt;/em&gt;.
However, if the program is &lt;code&gt;killed&lt;/code&gt; by the system, this can be mitigated by adjusting &lt;code&gt;BLOCK_SIZE&lt;/code&gt;.
Please have a look at the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/hl-compute.html&#34;&gt;computing model&lt;/a&gt; to understand how blocks are implemented and used in FORCE.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt;  The block size is a value, which was set in Level 2 Processing; thus the default block size aligns with the physical layout of the files, and can be considered optimal for reading speed. If you adjust it, it is recommended to use a block size, which is a fraction of the original block size (without remainder, e.g. 1/2 of the original value).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Quality screening&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This here is &lt;strong&gt;super-important,&lt;/strong&gt; and cannot be stressed enough!&lt;/p&gt;
&lt;p&gt;Without proper quality screening, the results will look like s#%$, trash-in, trash-out!
By default, FORCE screens for nodata values, various cloud types, cloud shadows, snow, sub-zero or saturated reflectance.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SCREEN_QAI = NODATA CLOUD_OPAQUE CLOUD_BUFFER CLOUD_CIRRUS CLOUD_SHADOW SNOW SUBZERO SATURATION
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, recognizing that cloud masks are never perfect, TSA offers an outlier detection routine.
This screens each pixel’s time series and might be used to remove undetected cloud, cloud shadow, or snow remnants.
The outlier detection is iteratively removing outliers until the time series noise is smaller than the given value.
Note however: this method might also remove some “valid” data points, e.g. mowing events in intensively managed grasslands.
This risk increases with decreasing data availability.&lt;/p&gt;
&lt;p&gt;Following the outlier removal, there is an inliner restoration.
If an observation (flagged as cloud, cloud shadow etc.) fits well into the time series trajectory, it will be restored.&lt;/p&gt;
&lt;p&gt;For our purpose, using this option is fine, thus, let&#39;s keep the default values:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ABOVE_NOISE = 3
BELOW_NOISE = 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Temporal extent, Sensor, Index&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To eventually generate a long term animation, let’s use 30 years of Landsat data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TIME_RANGE = 1990-01-01 2019-12-31
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;FORCE HLPS allows you to easily combine data from different sensors - provided that we only use mutually available bands.
For this tutorial, we are using data from the Landsat sensors:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;SENSORS = LND04 LND05 LND07 LND08
RESOLUTION = 30
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All processing in the TSA submodule is performed band-wise.
You can choose from a fairly long list of spectral bands and indices (see &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/tsa/param.html&#34;&gt;here&lt;/a&gt;).
HLPS will only read required bands to reduce I/O.
In order to generate a nice-looking and information-rich animation, we are using the three tasseled cap components:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;INDEX = TC-BRIGHT TC-GREEN TC-WET
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Interpolation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s define the interpolation parameters.
We wil be using the RBF interpolation to create a smoothed time series with 16-day interpolation steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;INTERPOLATE = RBF
INT_DAY = 16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are commonly using following widths for the gaussians. The width (in days) refers to full-width-at-half-maximum.
This generally works fine, but feel free to experiment here.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;RBF_SIGMA = 8 16 32
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cutoff parameter determines the width of the kernels.
It works similar to the confidence level in statistical tests, i.e. 0.95 means that we cut the kernel such that 95% of the Gaussian is retained.
Essentially, this paramter determines how many nodata values will remain in the time series.
You will have less nodata values with values closer to one.
However, the interpolated values will also be less reliable when the next valid observations are too far away (remember: trash-in, trash-out).
It also determines processing speed: more narrow kernels are faster.
Commonly, we are using the default value &lt;code&gt;0.95&lt;/code&gt;.
However, as our primary goal is to generate a nice-looking animation, we bump this parameter up:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;RBF_CUTOFF = 0.995
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;..and of course, we request outputting the interpolated time series:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OUTPUT_TSI = TRUE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Parallelization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;HLPS uses a computing model, which &lt;em&gt;streams&lt;/em&gt; the data.
Please have a detailed look the at &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/hl-compute.html&#34;&gt;computing model of HLPS&lt;/a&gt;.
Most often, generating interpolated time series (without doing anything else) is input-, or output-bound.
However, this strongly varies with data density, the number of requested indices, disc speed (SSD vs HDD, RAID or not, …), CPU clock speed, if you read/write from/to different (or the same) discs etc&amp;hellip;
The progress bar will tell you how much time is spent for reading, computing, and writing.
This helps you identify if your job is e.g. input-limited. You might want to adjust the settings accordingly (also note that you may have more or less CPUs than me).
Please also note: fairly often, inexperienced users tend to overdo parallel reads/writes beyond a value that is reasonable - if reading/writing doesn’t accelerate when you add more CPUs, this is likely the case (you might even slow down your job by overdoing I/O).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NTHREAD_READ = 8
NTHREAD_COMPUTE = 7
NTHREAD_WRITE = 3
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;processing&#34;&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Processing is straightforward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-higher-level /data/europe/interpolation/tsi.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;number of processing units: 280
 (active tiles: 28, chunks per tile: 10)
________________________________________
Progress:                        100.00%
Time for I/C/O:           054%/037%/008%
ETA:             00y 00m 00d 00h 00m 00s

________________________________________
Real time:       00y 00m 00d 00h 58m 41s
Virtual time:    00y 00m 00d 01h 32m 54s
Saved time:      00y 00m 00d 00h 34m 13s

________________________________________
Virtual I-time:  00y 00m 00d 00h 50m 30s
Virtual C-time:  00y 00m 00d 00h 34m 31s
Virtual O-time:  00y 00m 00d 00h 07m 53s

________________________________________
I-bound time:    00y 00m 00d 00h 23m 42s
C-bound time:    00y 00m 00d 00h 07m 10s
O-bound time:    00y 00m 00d 00h 00m 26s
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this, we do some postprocessing for simplified data handling, and to prepare the data for ingestion into the QGIS plugins.
First, we generate a mosaic:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/europe/interpolation
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mosaicking 3 products:
1 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.tif
2 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.tif
3 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.tif

mosaicking 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.tif
27 chips found.

mosaicking 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.tif
27 chips found.

mosaicking 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.tif
27 chips found.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we build a four-dimensional stack from the three tasseled cap components.
This stack is sorted by date, but interleaved by thematic band.
This data model is a prerequisite to the usage of the following QGIS plugins.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; For very long time series, &lt;code&gt;force-stack&lt;/code&gt; still seems a bit slow - but at least it works&amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd mosaic
force-stack *TCB*TSI.vrt *TCG*TSI.vrt *TCW*TSI.vrt 4D-Tasseled-Cap-TSI.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;file 1:
  /data/europe/interpolation/mosaic
  1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.vrt
  9000 4000 684
file 2:
  /data/europe/interpolation/mosaic
  1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.vrt
  9000 4000 684
file 3:
  /data/europe/interpolation/mosaic
  1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.vrt
  9000 4000 684

Same number of bands detected. Stacking by band.

Band 0001: 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.vrt band 1
Band 0002: 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.vrt band 1
Band 0003: 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.vrt band 1
Band 0004: 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.vrt band 2
Band 0005: 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.vrt band 2
Band 0006: 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.vrt band 2
Band 0007: 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.vrt band 3
Band 0008: 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.vrt band 3
Band 0009: 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.vrt band 3
...
Band 2050: 1990-2019_001-365_HL_TSA_LNDLG_TCB_TSI.vrt band 684
Band 2051: 1990-2019_001-365_HL_TSA_LNDLG_TCG_TSI.vrt band 684
Band 2052: 1990-2019_001-365_HL_TSA_LNDLG_TCW_TSI.vrt band 684
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For rapid display, we compute pyramids:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-pyramid 4D-Tasseled-Cap-TSI.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/europe/interpolation/mosaic/4D-Tasseled-Cap-TSI.vrt
computing pyramids for 4D-Tasseled-Cap-TSI.vrt
...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualization&#34;&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Layer Styling&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s drag&#39;n&#39;drop the &lt;code&gt;4D-Tasseled-Cap-TSI.vrt&lt;/code&gt; into &lt;strong&gt;QGIS&lt;/strong&gt;, and visualize the Tasseled Cap components of the 1st timestamp as RGB composite (using the &lt;code&gt;Multiband color&lt;/code&gt; renderer), i.e. we put the first three bands into the red (Brightness), green (Greenness), and blue (Wetness) channels for intuitive color interpretation.
The chosen stretch will later be applied to the animation as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Time Series Plots&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can conveniently look at the pixels&#39;s time series using the &lt;a href=&#34;https://raster-data-plotting.readthedocs.io/en/latest/&#34;&gt;Raster Data Plotting&lt;/a&gt; plugin (© &lt;a href=&#34;mailto:andreas.rabe@geo.hu-berlin.de&#34;&gt;Andreas Rabe&lt;/a&gt;).
I am using red, green, and blue lines for the Brigthness, Greenness, and Wetness components, respectively, and decrease the symbol size a bit.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rdp.jpg&#34; data-caption=&#34;Raster Data Plotting plugin&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rdp.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Raster Data Plotting plugin
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Time Series Animation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&#39;s use the &lt;a href=&#34;https://raster-timeseries-manager.readthedocs.io/en/latest/&#34;&gt;Raster Timeseries Manager&lt;/a&gt; plugin (© &lt;a href=&#34;mailto:andreas.rabe@geo.hu-berlin.de&#34;&gt;Andreas Rabe&lt;/a&gt;) to generate a web- or presentation-ready animation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;SETUP:&lt;/em&gt;&lt;/strong&gt;  For preparing the animation, go to the plugin&#39;s &lt;code&gt;System&lt;/code&gt; tab.
This needs to be done only once and will set up all necessary tools for saving the animation.
For this, you need to install &lt;a href=&#34;https://imagemagick.org/script/download.php&#34;&gt;ImageMagick&lt;/a&gt;, and then tell the plugin where the &lt;code&gt;imagemagick&lt;/code&gt; and &lt;code&gt;ffmpeg&lt;/code&gt; executables are located.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Simply select &lt;code&gt;4D-Tasseled-Cap-TSI&lt;/code&gt; in the &lt;code&gt;Timeseries&lt;/code&gt; field.
Now, you can already look at the animation (hit play &lt;code&gt;&amp;gt;&lt;/code&gt;).
While the animation is running, you can even pan and zoom in the map canvas.
IMHO, this is a great way of virtually exploring 4D data!&lt;/p&gt;
&lt;p&gt;In the temporal tab, you can set the temporal range for the animation.
For the animation shown below, I only use the last 5 years of data (otherwise the GIF will be too large).&lt;/p&gt;
&lt;p&gt;For saving the animation, go to the &lt;code&gt;Video Creator&lt;/code&gt; tab, and set an output directory.
The export process takes two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Export the frames as images.
This is done by pushing the movie tape button.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; The size of the exported images depends on the size of the map canvas.
This will have a direct effect on the size of the final animation.
Adjust the canvas size to your needs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate the animation by pushing the &lt;code&gt;MP4&lt;/code&gt; or &lt;code&gt;GIF&lt;/code&gt; button.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rtm.gif&#34; data-caption=&#34;Animation of interpolated Tasseled Cap time series&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-tsi-rtm.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Animation of interpolated Tasseled Cap time series
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;That&#39;s it. Have fun exploring your 4D data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Level 2 ARD</title>
      <link>https://davidfrantz.github.io/tutorials/force-ard/l2-ard/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-ard/l2-ard/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.2&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains what Analysis Ready Data are, and how to use the &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt; to generate them.&lt;/p&gt;
&lt;h2 id=&#34;what-are-levels&#34;&gt;&lt;strong&gt;What are Levels?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Remote sensing products are grouped in a hierarchical classification scheme. Level 0 data are the measurements taken onboard the satellite - they are not available to users. Level 1 data are radiometrically calibrated and georectified. Level 2 data most notably include some sort of atmospheric correction and probably other corrections like topographic correction. Level 3 data are temporal Level 2 aggregates, e.g. pixel based composites or statistical aggregations like multitemporal averages. Level 4 products are model output (classifications etc.), often derived from multi-temporal or multi-sensor measurements. I am defining Levels 1 and 2 as lower-level products and Levels 3 and above as higher-level products.&lt;/p&gt;
&lt;h2 id=&#34;what-are-analysis-ready-data&#34;&gt;&lt;strong&gt;What are Analysis Ready Data?&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Analysis Ready Data are satellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets (&lt;a href=&#34;http://ceos.org/ard&#34;&gt;CEOS&lt;/a&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Simply put, ARD are readily usable for any application without much further processing. In my opinion, such data need to be well corrected for atmospheric and other effects, have undergone a very good and aggressive cloud screening, are accompanied by pixel-based quality indicators (cloud masks but also other criteria), and are provided in a regular non-overlapping grid system without any redundancy in a single coordinate system (at least on the continental scale) in the form of data cubes.&lt;/p&gt;
&lt;p&gt;Just as a note: Although ARD are a huge step forward for increasing scientific and operational uptake from broader user groups, ARD do not represent the ultimate state of Analysis-Readiness. Thus, FORCE additionally provides means to generate highly Analysis Ready Data (hARD) and even highly Analysis Ready Data plus (hARD+), of which the latter can be directly ingested, analyzed, and interpreted in a GIS without ANY further processing. However, the scope of this tutorial is on plain ARD (don&#39;t get me wrong, ARD are great!).&lt;/p&gt;
&lt;h2 id=&#34;force-level-2-processing-system&#34;&gt;&lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt; is capable of generating such ARD.&lt;/p&gt;
&lt;p&gt;In principle, the same algorithm is applied to all supported sensors, although specific processing options are triggered or are only available for some sensors. Essentially, L2PS converts each Level 1 image to ARD specification. This includes three main processing steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;cloud and cloud shadow detection&lt;/li&gt;
&lt;li&gt;radiometric correction&lt;/li&gt;
&lt;li&gt;data cubing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For Sentinel-2, two additional options are implemented:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;resolution merging, i.e. increase the spatial resolution of the 20m bands to 10m&lt;/li&gt;
&lt;li&gt;co-registration with Landsat time series&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that most options and corrections can be switched off (e.g. atmospheric correction or data cubing) - this tutorial will however focus on our default parameterization for generating ARD.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-l2ps.jpg&#34; data-caption=&#34;FORCE Level 2 Processing System Workflow&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-l2ps.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    FORCE Level 2 Processing System Workflow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial walks you through the main parts, for more details, please refer to the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/index.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;make-sure-to-have-enough-horsepower&#34;&gt;&lt;strong&gt;Make sure to have enough horsepower&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Generating ARD is compute-heavy. Input and output need quite some disc space. And processes need sufficient RAM. Please do not expect this to work on a weak laptop or similar. It is mainly intended to run on sufficiently powerful servers or virtual machines. There are some ways to make most out of rigs where RAM is an issue (see parallel processing section below), but there is a limit. At least, you will need to have 8-8.5GB of RAM for processing a Sentinel-2 image.&lt;/p&gt;
&lt;h3 id=&#34;recommended-folder-structure&#34;&gt;&lt;strong&gt;Recommended folder structure&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Although not strictly required, I strongly suggest to use separate directories for Level 1 input data, Level 2 output data, parametrization, log files, temporary directory (if input are zip/tar.gz containers) and any other auxiliary data. So, let’s start by creating some directories, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir /data/force/level1
mkdir /data/force/level2
mkdir /data/force/param
mkdir /data/force/log
mkdir /data/force/misc
mkdir /data/force/temp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;the-parameter-file&#34;&gt;&lt;strong&gt;The parameter file&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The core element of L2PS is the parameter file. An empty Level 2 parameter file can be generated with&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-parameter /data/force/param LEVEL2 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;LEVEL2
An empty parameter file skeleton was written to
  /data/force/param/LEVEL2-skeleton.prm
Note that all parameters need to be given, even though some may not be used
with your specific parameterization.
You should rename the file, e.g. my-first-LEVEL2.prm.
Parameterize according to your needs and run with
force-level2 /data/force/param/my-first-LEVEL2.prm
 or for a single image:
force-l2ps image /data/force/param/my-first-LEVEL2.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The trailing &lt;code&gt;1&lt;/code&gt; means that descriptions for every parameter will be included. If you prefer a shorter parameter file, give a &lt;code&gt;0&lt;/code&gt; instead. The descriptions can also be found in the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/param.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s rename the file, and have a look:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mv /data/force/param/LEVEL2-skeleton.prm /data/force/param/l2ps.prm
head /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;++PARAM_LEVEL2_START++

# INPUT/OUTPUT DIRECTORIES
# ------------------------------------------------------------------------
# The file queue specifies, which images are to be processed. The full path
# to the file needs to be given. Do  not  paste  the content of the file queue
# into the parameter file. The file queue is mandatory for force-level2, but
# may be NULL for force-l2ps.
# Type: full file path
FILE_QUEUE = NULL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a simple text file. Lines beginning with &lt;code&gt;#&lt;/code&gt; are comments. All parameters are given in tag and value notation (&lt;code&gt;TAG = VALUE&lt;/code&gt;). The file can be edited with any text editor. However, make sure that you are using Unix End-of-Line &lt;code&gt;\n&lt;/code&gt;. MOST errors are because of parameter files with Windows End-of-Line &lt;code&gt;\r\n&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;parameterization&#34;&gt;&lt;strong&gt;Parameterization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Let&#39;s start to parameterize L2PS. Open the file in the text editor of your choice, e.g.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vi /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;1-input--output&#34;&gt;&lt;strong&gt;1) Input / Output&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The main input is a file queue that holds the full filepaths to all ingested images. All images that are enqueued (processing flag is &lt;code&gt;QUEUED&lt;/code&gt;) will be processed, all other are ignored. After processing, the flag will be set to &lt;code&gt;DONE&lt;/code&gt;. The &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/&#34;&gt;Sentinel-2 Level 1C tutorial&lt;/a&gt; explains how to use the FORCE Level 1 Archiving Suite (FORCE L1AS) to download, organize, and maintain a clean and consistent Sentinel-2 Level 1 data pool, as well as corresponding data queues needed for the Level 2 processing. There isn&#39;t a tutorial for Landsat yet, but it works similarly. Let&#39;s assume, we already have downloaded some images, the file queue is set like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FILE_QUEUE = /data/force/level1/queue.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we set the directories for output, logfiles and temporary data. The temp directory is mostly used for temporarily unpacking zip/tar.gz containers.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_LEVEL2 = /data/force/level2
DIR_LOG = /data/force/log
DIR_TEMP = /data/force/temp
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;2-digital-elevation-model&#34;&gt;&lt;strong&gt;2) Digital Elevation Model&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A Digital Elevation model is used to improve cloud and cloud shadow detection, atmospheric correction and to perform the topographic correction. The &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-dem/dem/&#34;&gt;DEM tutorial&lt;/a&gt; explains how to properly prepare a Digital Elevation Model (DEM). Let&#39;s assume, we already have prepared the DEM, it is set like this. Make sure to set the nodata value correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FILE_DEM = /data/force/misc/dem/srtm.vrt
DEM_NODATA = -32767
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;3-datacube-parameters&#34;&gt;&lt;strong&gt;3) Datacube parameters&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The datacube parameters, e.g. resolution, projection, tile size, block size, grid origin etc. are under full user control. As data cubing is an essential concept of FORCE, I highly recommend to read the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-datacube/datacube/&#34;&gt;Datacube tutorial&lt;/a&gt;, which explains what a datacube is, how it is parameterized, how you can find a POI, how to visualize the tiling grid, and how to conveniently display cubed data.&lt;/p&gt;
&lt;p&gt;Our parameter file already has some working defaults. You likely want to adjust them to your needs, but for starters, let&#39;s take the default values.&lt;/p&gt;
&lt;h3 id=&#34;4-radiometric-correction&#34;&gt;&lt;strong&gt;4) Radiometric correction&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default parameter file already has all radiometric corrections enabled, and this is the setup I commonly use for generating ARD. This includes atmospheric correction with multiple scattering effects, image-based AOD estimation, topographic correction, adjacency effect correction, and nadir BRDF correction. The only thing that needs to be changed (and only if processing Landsat data) is the parameterization of the water vapor correction. Please see the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-dem/dem/&#34;&gt;Water Vapor Database tutorial&lt;/a&gt; for instructions on how to prepare/download the Water Vapor Database. The directory that contains this database needs to be like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DIR_WVPLUT = /data/force/misc/wvdb
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;5-cloud-detection&#34;&gt;&lt;strong&gt;5) Cloud detection&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default parameter file already has meaningful values for the cloud correction. I usually don&#39;t tweak the Fmask parameters. You can probably change the maximum cloud cover parameters to your liking. The &lt;code&gt;MAX_CLOUD_COVER_FRAME&lt;/code&gt; parameter cancels the processing of images that exceed the given threshold. The processing will be canceled right after cloud detection and thus saves quite some processing time. In my opinion, heavily clouded images are most often of little use, and even if cloud detection flags some pixels as &amp;ldquo;clear&amp;rdquo;, they are usually somewhat contaminated, e.g. in transition zones from clear-sky to cloud.. Therefore, I commonly do not go up to 100%. The &lt;code&gt;MAX_CLOUD_COVER_TILE&lt;/code&gt; parameter is similar, but it works on a per tile basis. It suppresses the output for chips (tiled image) that exceed the given threshold.&lt;/p&gt;
&lt;h3 id=&#34;6-resolution-merge&#34;&gt;&lt;strong&gt;6) Resolution merge&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This parameter defines the method used for improving the spatial resolution of Sentinel-2’s 20m bands to 10m. It defaults to the &lt;a href=&#34;https://ieeexplore.ieee.org/document/7452606&#34;&gt;ImproPhe code&lt;/a&gt;, which is a data fusion option with both decent performance and quality. Let&#39;s keep this method, but feel free to try the other options.&lt;/p&gt;
&lt;h3 id=&#34;7-co-registration&#34;&gt;&lt;strong&gt;7) Co-Registration&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since v. 3.0, FORCE is able to perform a co-registration of Sentinel-2 images with Landsat time series. For starters, we will not use this option. A separate tutorial is planned asap that will explain how to do this.&lt;/p&gt;
&lt;h3 id=&#34;8-parallel-processing&#34;&gt;&lt;strong&gt;8) Parallel Processing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;FORCE L2PS uses a nested parallelization strategy. The main parallelization level is multiprocessing: individual images are processed simultaneously (the box in the workflow figure). Each process can additionally use multithreading, which means that each image can be additionally processed parallely. The multiplication of both shouldn&#39;t exceed the number of threads your system supports.&lt;/p&gt;
&lt;p&gt;I recommend to use as many processes, and as few threads as possible. However, a mild mix may be beneficial, e.g. 2 threads per process. If processing only a few (or one) image, increase the multithreading ratio accordingly. This can speed up the work significantly. If RAM is too small, inncrease the multithreading ratio accordingly. If there isn&#39;t enough RAM to support all processes, some images will fail due to insufficient memory.&lt;/p&gt;
&lt;p&gt;To prevent an I/O jam at startup (by reading / extracting a lot of data simultaneously), a delay (in seconds) might be necessary: a new process waits for some seconds before starting. The necessary delay (or none) is dependent on your system’s architecture (I/O speed etc), on sensor to be processed, and whether packed archives or uncompressed images are given as input.&lt;/p&gt;
&lt;p&gt;Please note that I cannot recommend useful default settings. This is extremely dependent on your rig&#39;s setup (# of CPUs, RAM, I/O speed, parallel disc access etc.) and on what exactly you are doing (e.g. Sentinel-2 has higher ressource requirements compared to Landsat, are the input images extracted or still packed in zip/tar.gz containers, enabling/disabling certain processing options have an effect, too).&lt;/p&gt;
&lt;p&gt;Please have a look at these two setups (click to enlarge). The plots illustrate how the work (of processing the same 8 images) is being spread to CPUs and threads, how the delay works, and how the processes consume RAM (highly idealized - actually, the memory footprint varies across runtime).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Good setup&lt;/th&gt;
&lt;th&gt;Bad setup&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;4 processes with 2 threads each&lt;/td&gt;
&lt;td&gt;8 processes with 1 thread each&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RAM is large enough to support this many processes&lt;/td&gt;
&lt;td&gt;RAM is not large enough to support this many processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-good.jpg&#34; &gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-good.jpg&#34; alt=&#34;&#34; width=&#34;200&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;td&gt;


  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-bad.jpg&#34; &gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-cpu-ram-l2-bad.jpg&#34; alt=&#34;&#34; width=&#34;200&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In my case, I am running on a bare-metal Ubuntu server with 32 CPUs / 64 threads, 500GB RAM (way more than sufficient), and a RAID6 HDD file system that is directly attached to the server. Both my Landsat and Sentinel-2 input images are still packed. I am using these parameters:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NPROC = 32
NTHREAD = 2
DELAY = 5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;9-output-options&#34;&gt;&lt;strong&gt;9) Output options&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The default output options are already my usual setup for ARD generation. The output files will be stored as compressed GeoTiff images with internal blocks for partial access. Note that metadata are written to the FORCE domain, thus they only show up if you look into all metadata domains, e.g.&lt;/p&gt;
&lt;p&gt;The Bottom-of-Atmosphere reflectance product and the Quality Assurance Information are written by default - and they can&#39;t be disabled. I typically generate additional quicklooks (&lt;code&gt;OUTPUT_OVV&lt;/code&gt;). If you want to generate pixel based composites in the next step, you should additionally output the &lt;code&gt;OUTPUT_DST&lt;/code&gt;, &lt;code&gt;OUTPUT_VZN&lt;/code&gt;, and &lt;code&gt;OUTPUT_HOT&lt;/code&gt; products. The &lt;code&gt;OUTPUT_AOD&lt;/code&gt; and &lt;code&gt;OUTPUT_WVP&lt;/code&gt; products are not used by any higher level submodule - they are only useful for validation purposes.&lt;/p&gt;
&lt;h2 id=&#34;processing&#34;&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Once the parameter file is finished, processing is pretty straight forward. Simply feed the parameter file to &lt;code&gt;force-level2&lt;/code&gt;. A progress bar keeps you updated about the ETA, the number of completed, running and waiting processes.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level2 /data/force/param/l2ps.prm
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;47 images enqueued. Start processing with 32 CPUs

Computers / CPU cores / Max jobs to run
1:local / 64 / 32

Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
ETA: 12046s Left: 45 AVG: 280.00s  local:32/2/100%/596.5s 
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;logfile&#34;&gt;&lt;strong&gt;Logfile&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;After processing, I recommend to check the logfiles, which we have written to &lt;code&gt;/data/force/log&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/force/log | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_OPER_MSI_L1C_TL_SGS__20160310T160000_A003736_T33JYG_N02.01
S2A_OPER_MSI_L1C_TL_SGS__20160409T141153_A004165_T33JYG_N02.01
S2A_OPER_MSI_L1C_TL_SGS__20160827T135818_A006167_T33JYG_N02.04
S2A_OPER_MSI_L1C_TL_SGS__20160916T135429_A006453_T33JYG_N02.04
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logfiles report the percentage of data cover (how many pixels are not no-data), water cover, snow cover and cloud cover. Then, aerosol optical depth @ 550 nm (scene average), and the number of dark targets for retrieving aerosol optical depth (over water/vegetation) are printed. Then, the number of products written (number of tiles), and a supportive success indication is printed. In the case the overall cloud coverage is higher than allowed, the image is skipped. The processing time (real time) is appended at the end.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/force/log/* | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_OPER_MSI_L1C_TL_SGS__20160310T160000_A003736_T33JYG_N02.01: dc:  99.95%. wc:  18.04%. sc:   0.07%. cc:   7.13%. AOD: 0.1129. # of targets: 240/42. 25 product(s) written. Success! Processing time: 14 mins 35 secs
S2A_OPER_MSI_L1C_TL_SGS__20160409T141153_A004165_T33JYG_N02.01: dc: 100.00%. wc:  18.39%. sc:   0.07%. cc:   0.08%. AOD: 0.1455. # of targets: 25/43. 25 product(s) written. Success! Processing time: 15 mins 19 secs
S2A_OPER_MSI_L1C_TL_SGS__20160827T135818_A006167_T33JYG_N02.04: dc: 100.00%. wc:  18.43%. sc:   0.10%. cc:   0.11%. AOD: 0.1208. # of targets: 74/0. 25 product(s) written. Success! Processing time: 13 mins 50 secs
S2A_OPER_MSI_L1C_TL_SGS__20160916T135429_A006453_T33JYG_N02.04: dc: 100.00%. wc:   1.78%. sc:   2.85%. cc: 100.00%. Skip. Processing time: 12 mins 17 secs
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;output-format&#34;&gt;&lt;strong&gt;Output format&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For all details on the output format, please refer to the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/lower-level/level2/format.html#metadata&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The output data are organized in data cubes. The tiles manifest as directories in the file system, and the images are stored within. This is decribed in more detail in the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-datacube/datacube/&#34;&gt;Datacube tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Basically, for each tile, you get a time series of square image chips that always show the same extent:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-results.jpg&#34; data-caption=&#34;Data Cube of Landsat 7/8 and Sentinel-2 A/B Level 2 ARD. A two-month period of atmospherically corrected imagery acquired over South-East Berlin, Germany, is shown here.&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l2-ard-results.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Data Cube of Landsat 7/8 and Sentinel-2 A/B Level 2 ARD. A two-month period of atmospherically corrected imagery acquired over South-East Berlin, Germany, is shown here.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Each dataset consists of a &lt;code&gt;BOA&lt;/code&gt; and &lt;code&gt;QAI&lt;/code&gt; product, which are Bottom-of-Atmosphere reflectance and Quality Assurance Information. Depending on parameterization ,there are more products, e.g. &lt;code&gt;OVV&lt;/code&gt; for image overviews (quicklooks).&lt;/p&gt;
&lt;p&gt;The reflectance products are multi-band images and consist of 6 bands for Landsat (Landsat legacy bands), and 10 bands for Sentinel-2 (land surface bands). All bands are provided at the same spatial resolution, typically 30m for Landsat and 10m for Sentinel-2.&lt;/p&gt;
&lt;p&gt;QAI are provided bit-wise for each pixel. QAI are essential for making your analyses a success, therefore, please have a look at the &lt;a href=&#34;https://davidfrantz.github.io/tutorials/force-qai/qai/&#34;&gt;Quality Bits tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Metadata are written to all output products. Note that FORCE-specific metadata will be written to the FORCE domain, and thus are probably not visible unless the FORCE domain (or all domains) are specifically requested:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{bash,&#34;&gt;gdalinfo -mdd all /data/force/level2/X0007_Y0007/20170424_LEVEL2_SEN2A_BOA.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Driver: GTiff/GeoTIFF
Files: /data/force/level2/X0007_Y0007/20170424_LEVEL2_SEN2A_BOA.tif
Size is 3000, 3000
Coordinate System is:
PROJCS[&amp;quot;WGS 84 / UTM zone 33S&amp;quot;,
    GEOGCS[&amp;quot;WGS 84&amp;quot;,
        DATUM[&amp;quot;WGS_1984&amp;quot;,
        
...

Band 10 Block=3000x300 Type=Int16, ColorInterp=Undefined
  Description = SWIR2
  NoData Value=-9999
  Metadata (FORCE):
    Date=2017-04-24T08:26:01.0Z
    Domain=SWIR2
    Scale=10000.000
    Sensor=SEN2A
    Wavelength=2.202
    Wavelength_unit=micrometers
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ard-now-what&#34;&gt;&lt;strong&gt;ARD, now what?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;FORCE provides a lot of functionality to further process the generated ARD into hARD or hARD+ products, e.g. using pixel-based compositing or time series analyses.&lt;/p&gt;
&lt;p&gt;Please see the &lt;a href=&#34;https://force-eo.readthedocs.io/en/latest/components/higher-level/index.html&#34;&gt;Higher Level processing options&lt;/a&gt; in the documentation. Some more tutorials are planned, which deal with all these options.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Processing Masks</title>
      <link>https://davidfrantz.github.io/tutorials/force-masks/masks/</link>
      <pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-masks/masks/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains how to generate and use processing masks in the &lt;strong&gt;FORCE Higher Level Processing System&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;what-are-processing-masks&#34;&gt;&lt;strong&gt;What are processing masks?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;strong&gt;FORCE Higher Level Processing System&lt;/strong&gt;, processing masks can be used to restrict processing and analysis to certain pixels of interest. The masks need to be in datacube format, i.e. they need to be raster images in the same grid as all the other data. The masks can - but don’t need to - be in the same directory as the other data. The masks should be binary images. The pixels that have a mask value of 0 will be skipped.&lt;/p&gt;
&lt;h2 id=&#34;what-is-the-advantage-of-using-processing-masks&#34;&gt;&lt;strong&gt;What is the advantage of using processing masks?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Processing masks speed up processing.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each processing unit (block within the tile), the analysis mask is read first. If no valid pixel is in there, all the other data are not input, and the block is skipped. As an example, when processing a country like Japan, and provide a land mask, you can speed up processing significantly as many blocks are skipped entirely.&lt;/li&gt;
&lt;li&gt;On the pixel level, invalid pixels are skipped, too. This is especially beneficial for CPU-heavy tasks, e.g. machine learning predictions. As an example, when computing a tree species classification, you can speed up processing substantially if you provide a forest masks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Processing masks decrease data volume substantially.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the processed products, the pixels of no interest have a nodata value. As all FORCE output is compressed (unless you choose to output in ENVI format; I don’t recommend to do this), the compression kicks in nicely if you have used processing masks. You can easily decrease data volume by several factors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Processing masks facilitate analyzing the processed data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the processed products, the pixels of no interest have a nodata value. Thus, you don’t need to sort the pixels on your own, e.g. computing confusion matrices and classification accuracy is more straightforward to implement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generate-processing-masks&#34;&gt;&lt;strong&gt;Generate processing masks&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;option-1-from-shapefile-to-mask&#34;&gt;&lt;strong&gt;Option 1: from shapefile to mask&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;FORCE&lt;/strong&gt; comes with a program to generate processing masks from a shapefile:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-cube
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-cube input-file output-dir resample resolution
       input-file: the file you want to cube
       output-dir: the directory you want to store the cubes;
                   datacube-definition.prj needs to exist in there
       resample:   resampling method
                   (1) any GDAL resampling method for raster data, e.g. cubic
                   (2) rasterize for vector data
       resolution: the resolution of the cubed data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;force-cube&lt;/code&gt; imports raster or vector data into the datacube format needed by &lt;strong&gt;FORCE&lt;/strong&gt;. The output directory needs to contain a copy of the datacube definition (see datacube tutorial).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rasterize&lt;/code&gt; resampling option rasterizes polygon vector geometries. It burns the occurence of the geometry into a raster image, i.e. it assigns the value &lt;em&gt;1&lt;/em&gt; to all cells that are covered by a geometry, &lt;em&gt;0&lt;/em&gt; if not. The resulting masks are compressed GeoTiff images. Do not worry about data volume when converting from vector to raster data, because the compression rate is extremely high.&lt;/p&gt;
&lt;p&gt;In the following example, we generate a processing mask for the administrative area of Vienna, Austria.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-cube vienna.shp /data/Dagobah/edc/misc/mask rasterize 10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0...10...20...30...40...50...60...70...80...90...100 - done.
0...10...20...30...40...50...60...70...80...90...100 - done.
0...10...20...30...40...50...60...70...80...90...100 - done.
0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, Vienna is covered by four tiles, a cubed GeoTiff was generated in each tile:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/misc/mask/X*/vienna.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/misc/mask/X0077_Y0058/vienna.tif
/data/Dagobah/edc/misc/mask/X0077_Y0059/vienna.tif
/data/Dagobah/edc/misc/mask/X0078_Y0058/vienna.tif
/data/Dagobah/edc/misc/mask/X0078_Y0059/vienna.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For speedy visuailzation, build overviews and pyramids:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/Dagobah/edc/misc/mask
force-pyramid /data/Dagobah/edc/misc/mask/mosaic/vienna.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mosaicking vienna.tif
4 chips found.
computing pyramids for vienna.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-mask-vector.jpg&#34; data-caption=&#34;Mask of Vienna generated from a shapefile. Overlayed with the processing grid in green&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-mask-vector.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Mask of Vienna generated from a shapefile. Overlayed with the processing grid in green
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;option-2-from-raster-to-mask&#34;&gt;&lt;strong&gt;Option 2: from raster to mask&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;As of now, &lt;strong&gt;FORCE&lt;/strong&gt; does not come with a handy tool to generate masks from a raster image with continuous values (this is on my to-do list though). However, you can follow this recipe to accomplish this.&lt;/p&gt;
&lt;p&gt;In the example given below, our input image is a multiband continuous fields dataset, which gives the percentages of built-up land (urban), high vegetation (trees), and low vegetation (grass, agriculture). Point 1) may be skipped if the data are already in datacube format, which is the case in this example.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the data are not already in the datacube format, use &lt;code&gt;force-cube&lt;/code&gt; to import the data (see the usage above). Use a raster resampling option to trigger the raster import, e.g. &lt;code&gt;cubic&lt;/code&gt; (bc it&#39;s all about cubes, eh?).&lt;/li&gt;
&lt;li&gt;Go to the parent directory of the cubed images (this is important for the next point), and generate a list with the filenames:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /data/Jakku/germany-LC/pred
ls X*/CONFIELD_MLP.tif &amp;gt; files.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;In this example, the image covers 597 tiles:
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l files.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;597 files.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head files.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;X0052_Y0045/CONFIELD_MLP.tif
X0052_Y0046/CONFIELD_MLP.tif
X0052_Y0047/CONFIELD_MLP.tif
X0052_Y0048/CONFIELD_MLP.tif
X0052_Y0049/CONFIELD_MLP.tif
X0052_Y0050/CONFIELD_MLP.tif
X0052_Y0051/CONFIELD_MLP.tif
X0052_Y0052/CONFIELD_MLP.tif
X0052_Y0053/CONFIELD_MLP.tif
X0053_Y0045/CONFIELD_MLP.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Generate the masks using a command similar to the example below. The 1st part of the command uses the list from point 2), and parallely calls the command in parentheses &lt;code&gt;&amp;quot;...&amp;quot;&lt;/code&gt;. The curly braces &lt;code&gt;{//}&lt;/code&gt; replace the input image with its dirname, i.e. with the tile ID. A directory for the tile is generated if it is not already existing. The &lt;code&gt;gdal_calc.py&lt;/code&gt; command handles simple raster algebra. The &lt;code&gt;-A&lt;/code&gt; and &lt;code&gt;--A_band&lt;/code&gt; options specify the image and band on which to operate the calculation specified by &lt;code&gt;--calc&lt;/code&gt; (in our input image, the tree percentage is in band 2). A binary image (= mask) will be generated, wherein all pixels larger than 3000 (i.e. 30%) are set to &lt;em&gt;1&lt;/em&gt;. The &lt;code&gt;--creation-option&lt;/code&gt; parameters are options that specify compression etc. The blocksize parameters should best reflect the blocksize used for the datacube (see datacube tutorial). &lt;em&gt;As said before, a tool for this will likely be implemented in a not-so-far future version of FORCE.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;parallel -a files.txt &amp;quot;mkdir -p /data/Dagobah/edc/misc/mask/{//}; gdal_calc.py -A {} --A_band=2 --outfile=/data/Dagobah/edc/misc/mask/{//}/forest-mask.tif --calc=&#39;(A&amp;gt;3000)&#39; --NoDataValue=255 --type=Byte --format=GTiff --creation-option=&#39;COMPRESS=LZW&#39; --creation-option=&#39;PREDICTOR=2&#39; --creation-option=&#39;NUM_THREADS=ALL_CPUS&#39; --creation-option=&#39;BIGTIFF=YES&#39; --creation-option=&#39;BLOCKXSIZE=3000&#39; --creation-option=&#39;BLOCKYSIZE=300&#39;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
0 .. 10 .. 20 .. 30 .. 40 .. 50 .. 60 .. 70 .. 80 .. 90 .. 100 - Done
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have one cubed mask for each input image in the mask directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/misc/mask/X*/forest-mask.tif | wc -l
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;597
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For speedy visuailzation, build overviews and pyramids:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/Dagobah/edc/misc/mask
force-pyramid /data/Dagobah/edc/misc/mask/mosaic/forest-mask.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;mosaicking forest-mask.tif
597 chips found.
computing pyramids for forest-mask.vrt
0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-mask-raster.jpg&#34; data-caption=&#34;Forest mask generated from continuous raster input. Overlayed with the processing grid in green&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-mask-raster.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Forest mask generated from continuous raster input. Overlayed with the processing grid in green
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;use-processing-masks&#34;&gt;&lt;strong&gt;Use processing masks&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Processing masks can easily be used in &lt;code&gt;force-higher-level&lt;/code&gt; by setting the &lt;code&gt;DIR_MASK&lt;/code&gt; and &lt;code&gt;BASE_MASK&lt;/code&gt; parameters in the parameter file. They are the parent directory of the cubed masks, and the basename of the masks, respectively. To use the Vienna mask from above:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DIR_MASK = /data/Dagobah/edc/misc/mask&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;BASE_MASK = vienna.tif&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Sentinel-2 Level 1C</title>
      <link>https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/</link>
      <pubDate>Sat, 15 Feb 2020 15:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-level1-s2/sentinel-2-l1c/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains how to use the FORCE Level 1 Archiving Suite (FORCE L1AS) to download, organize, and maintain a clean and consistent Sentinel-2 Level 1 data pool, as well as corresponding data queues needed for the Level 2 processing.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ESA provides an application programming interface (API) for data query and automatic download (see &lt;a href=&#34;https://scihub.copernicus.eu/twiki/do/view/SciHubUserGuide/BatchScripting?redirectedfrom=SciHubUserGuide.8BatchScripting&#34;&gt;here&lt;/a&gt;). Based on some user-defined parameters (coordinates etc.) &lt;strong&gt;FORCE L1AS&lt;/strong&gt; pulls a metadata report from the Copernicus API Hub. Each hit is compared
with the local data holdings you already downloaded. If a new file is sitting on ESA&#39;s end, the missing image is downloaded. A file queue is generated and updated accordingly - which is the main input to the &lt;strong&gt;FORCE Level 2 Processing System&lt;/strong&gt;.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-l1sen2.png&#34; data-caption=&#34;Sentinel-2 downloader in the FORCE Level 1 Archiving Suite&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-l1sen2.png&#34; alt=&#34;&#34; width=&#34;750&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sentinel-2 downloader in the &lt;strong&gt;FORCE Level 1 Archiving Suite&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;setup&#34;&gt;&lt;strong&gt;Setup&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Now, the first step is to get access to ESA&#39;s data and services. For that, you need an account. If you don&#39;t have one, register &lt;a href=&#34;https://scihub.copernicus.eu/dhus/#/self-registration&#34;&gt;here&lt;/a&gt;. It&#39;s free.&lt;/p&gt;
&lt;p&gt;Then, your credentials must be made available to &lt;strong&gt;FORCE L1AS&lt;/strong&gt; to be able to request and receive ESA data. On your machine, your login credentials must be placed in a hidden file &lt;code&gt;.scihub&lt;/code&gt; in your home directory. I advise to only give user reading rights to this file. The user name goes in the first line, the password in the second line. Please note that special characters might be problematic. Also note, if you generate this file from a Windows machine, the Windows EOL character will cause problems.&lt;/p&gt;
&lt;h2 id=&#34;download-some-data&#34;&gt;&lt;strong&gt;Download some data&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;instructions&#34;&gt;&lt;strong&gt;Instructions&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;After setting up your account, you should be able to download Sentinel-2 data via &lt;strong&gt;FORCE L1AS&lt;/strong&gt;. As with any other &lt;strong&gt;FORCE&lt;/strong&gt; program, you can display short usage instructions by executing the program without any parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-level1-sentinel2 Level-1-Datapool queue Boundingbox
                   starttime endtime min-cc max-cc [dry]

  Level-1-Datapool
  An existing directory, your files will be stored here

  queue
  Downloaded files are appended to a file queue, which is needed for
  the Level 2 processing. The file doesn&#39;t need to exist. If it exists,
  new lines will be appended on successful ingestion

  Boundingbox
  The coordinates of your study area: &amp;quot;X1/Y1,X2/Y2,X3/Y3,...,X1/Y1&amp;quot;
  The box must be closed (first X/Y = last X/Y). X/Y must be given as
  decimal degrees with negative values for West and South coordinates.
  Note that the box doesn&#39;t have to be square, you can specify a polygon

  starttime endtime
  Dates must be given as YYYY-MM-DD

  min-cc max-cc
  The cloud cover range must be given in %

  dry will trigger a dry run that will only return the number of images
  and their total data volume

  Your ESA credentials must be placed in /home/frantzda/.scihub
    First line: User name
    Second line: Password, special characters might be problematic
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;dry-run&#34;&gt;&lt;strong&gt;Dry run&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Please note that &lt;strong&gt;FORCE&lt;/strong&gt; won&#39;t check that there is enough space on your hard disc. If you don&#39;t dare to download all data straight away, there is a dry run option implemented that only checks how much data would be downloaded with the parameters you provided. This is given by the optional &lt;code&gt;dry&lt;/code&gt; keyword at the end of the command line. The following query asks for all data in July 2019 with a maximum cloud coverage of 50%. The region of interest here is a rather small area in Zambia&#39;s Northwestern Province, depicting a large copper mine in the Miombo forest.&lt;/p&gt;
&lt;p&gt;The single most frequent error here is the specification of the coordinates. If you don&#39;t receive any data for your study area, or end up somewhere else entirely, double check that the coordinates are given in the correct order.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X = Longitude&lt;/li&gt;
&lt;li&gt;Y = Latitude&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50 dry
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:36:36 - Found 13 S2A/B files.
13 Sentinel-2 A/B L1C files available
5.19094 GB data volume available
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;download&#34;&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The actual download is triggered by omitting the &lt;code&gt;dry&lt;/code&gt; option. &lt;strong&gt;FORCE L1AS&lt;/strong&gt; downloads all data that match the parameters provided - and which weren&#39;t downloaded before. Note that the program checks against the files on the disc (not the file queue). Each downloaded image is unzipped after the download. If both steps were successful, the image is appended to the file queue.&lt;/p&gt;
&lt;p&gt;Do not wonder if FORCE tells you that it has found exactly 100 S2A/B files. The ESA API Hub only allows to retrieve metadata for 100 products. Thus, FORCE iterates through the pages until no more image can be retrieved.&lt;/p&gt;
&lt;p&gt;Please note that download speed varies considerably..&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:37:03 - Found 13 S2A/B files.
2020-02-15_15:37:03 - Found 13 S2A/B files on this page.
/data/Dagobah 100%[===================&amp;gt;] 729.50M  32.4MB/s    in 23s     
/data/Dagobah 100%[===================&amp;gt;] 271.14M  30.5MB/s    in 7.0s    
/data/Dagobah 100%[===================&amp;gt;] 742.98M  29.8MB/s    in 24s     
/data/Dagobah 100%[===================&amp;gt;] 266.53M  28.1MB/s    in 11s     
/data/Dagobah 100%[===================&amp;gt;] 732.80M  30.7MB/s    in 20s     
/data/Dagobah 100%[===================&amp;gt;] 224.77M  69.2MB/s    in 3.2s    
/data/Dagobah 100%[===================&amp;gt;] 730.90M  81.6MB/s    in 9.6s    
/data/Dagobah 100%[===================&amp;gt;] 268.45M  42.5MB/s    in 7.9s    
/data/Dagobah 100%[===================&amp;gt;] 704.98M  45.8MB/s    in 21s     
/data/Dagobah 100%[===================&amp;gt;] 258.02M  47.5MB/s    in 5.9s    
/data/Dagobah 100%[===================&amp;gt;] 754.09M  61.7MB/s    in 13s     
/data/Dagobah 100%[===================&amp;gt;] 259.80M  66.5MB/s    in 4.3s    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the Level 1 Datapool, there is now the file queue, as well as a directory for each MGRS tile that was retrieved.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;T35LLG  zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the directories, there are the unzipped images.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C/T*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that for Sentinel-2, the compression is realized in the image data, not in the container, thus unzipping the data does not inflate the file size (much).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;du -h -d 1 /data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;5.9G	/data/Dagobah/S2L1C/T35LLG
5.9G	/data/Dagobah/S2L1C
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file queue is holding the full filepaths to all ingested images. This is the main input to &lt;strong&gt;force-level2&lt;/strong&gt;. Each image is in a separate line. A processing-state flag determines if the image is enqueued for Level 2 processing - or was already processed and will be ignored next time. This flag is either &lt;code&gt;QUEUED&lt;/code&gt; or &lt;code&gt;DONE&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/S2L1C/zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE QUEUED
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;download-some-more-data&#34;&gt;&lt;strong&gt;Download some more data&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Downloading more data is easy. You can use the same datapool, and the same file queue for this. Images are only downloaded if they weren&#39;t downloaded yet. Thus, you can e.g. change the boundingbox, time frame or cloud coverage, and only download data that was not covered by the previous run. In the following example, the Eastern X-Coordinates were increased by 1 degree.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,26.94/-12.46,26.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2019-07-01 2019-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:43:57 - Found 26 S2A/B files.
2020-02-15_15:43:57 - Found 26 S2A/B files on this page.
/data/Dagobah 100%[===================&amp;gt;]  54.08M  43.4MB/s    in 1.2s    
/data/Dagobah 100%[===================&amp;gt;] 794.27M  73.7MB/s    in 11s     
/data/Dagobah 100%[===================&amp;gt;]  57.66M  61.0MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 782.06M  80.1MB/s    in 10s     
/data/Dagobah 100%[===================&amp;gt;]  49.95M  52.0MB/s    in 1.0s    
/data/Dagobah 100%[===================&amp;gt;] 555.54M  85.9MB/s    in 6.8s    
/data/Dagobah 100%[===================&amp;gt;]  52.83M  57.7MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 788.67M  79.2MB/s    in 12s     
/data/Dagobah 100%[===================&amp;gt;]  48.47M  52.4MB/s    in 0.9s    
/data/Dagobah 100%[===================&amp;gt;] 779.62M  81.5MB/s    in 9.4s    
/data/Dagobah 100%[===================&amp;gt;]  58.56M  50.9MB/s    in 1.1s    
/data/Dagobah 100%[===================&amp;gt;] 781.21M  54.6MB/s    in 17s     
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the 26 available images, only 13 were retrieved, 13 were already downloaded before. There are now several directories with different MGRS tiles in your Level 1 Datapool.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/S2L1C/T*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG:
S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE

/data/Dagobah/S2L1C/T35LMG:
S2A_MSIL1C_20190707T080611_N0207_R078_T35LMG_20190707T100942.SAFE
S2A_MSIL1C_20190710T081611_N0208_R121_T35LMG_20190710T103430.SAFE
S2A_MSIL1C_20190717T080611_N0208_R078_T35LMG_20190717T110132.SAFE
S2A_MSIL1C_20190720T081611_N0208_R121_T35LMG_20190720T134157.SAFE
S2A_MSIL1C_20190727T080611_N0208_R078_T35LMG_20190727T115444.SAFE
S2A_MSIL1C_20190730T081611_N0208_R121_T35LMG_20190730T103748.SAFE
S2B_MSIL1C_20190702T080619_N0207_R078_T35LMG_20190702T115117.SAFE
S2B_MSIL1C_20190705T081609_N0207_R121_T35LMG_20190705T110801.SAFE
S2B_MSIL1C_20190712T080619_N0208_R078_T35LMG_20190712T110120.SAFE
S2B_MSIL1C_20190715T081609_N0208_R121_T35LMG_20190715T121541.SAFE
S2B_MSIL1C_20190722T080619_N0208_R078_T35LMG_20190722T110547.SAFE
S2B_MSIL1C_20190725T081609_N0208_R121_T35LMG_20190725T120407.SAFE
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new files were appended to the file queue, too.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/S2L1C/zambia.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LLG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LLG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LLG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LLG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LLG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LLG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LLG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LLG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LLG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LLG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LLG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LLG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LLG_20190702T115117.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190730T081611_N0208_R121_T35LMG_20190730T103748.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190727T080611_N0208_R078_T35LMG_20190727T115444.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190725T081609_N0208_R121_T35LMG_20190725T120407.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190722T080619_N0208_R078_T35LMG_20190722T110547.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190720T081611_N0208_R121_T35LMG_20190720T134157.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190717T080611_N0208_R078_T35LMG_20190717T110132.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190715T081609_N0208_R121_T35LMG_20190715T121541.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190712T080619_N0208_R078_T35LMG_20190712T110120.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190710T081611_N0208_R121_T35LMG_20190710T103430.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2A_MSIL1C_20190707T080611_N0207_R078_T35LMG_20190707T100942.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190705T081609_N0207_R121_T35LMG_20190705T110801.SAFE QUEUED
/data/Dagobah/S2L1C/T35LMG/S2B_MSIL1C_20190702T080619_N0207_R078_T35LMG_20190702T115117.SAFE QUEUED
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;setting-up-a-scheduled-download&#34;&gt;&lt;strong&gt;Setting up a scheduled download&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The same logic can be used to set up a scheduler for downloading your data at regular intervals. For example, a daily cronjob can be installed to retrieve all data covering your study area. A cronjob is installed by adding lines to the cronjob file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;crontab -e
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your FORCE installation is not in a standard search path, you need to define the PATH variable, and include the path where FORCE is installed. Then, use the command line from above and schedule it with cron notation. Following line will start the download at 3:00 AM each day. &lt;em&gt;Replace YOURNAME with your user name.&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PATH=/home/YOURNAME/bin:/usr/bin:/bin:/usr/local/bin
0 3 * * * force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,26.94/-12.46,26.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2018-07-01 2018-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;long-term-archive&#34;&gt;&lt;strong&gt;Long Term Archive&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In September 2019, ESA has activated the Long Term Archive (LTA) to roll out old (and potentially infrequently used) data products from the online storage system to offline storage. For details, see &lt;a href=&#34;https://scihub.copernicus.eu/userguide/LongTermArchive&#34;&gt;here&lt;/a&gt;. As of now (the following numbers might change in the future), the last year of data shall stay online, and is immediately ready for download. Offline data may be pulled from offline to online storage upon request. The data retrieval shall happen within 24h and the products shall stay online for 3 days. If they were not downloaded within this time period, they need to be pulled again. A user quota is implemented to prevent users from pulling the entire archive - unfortunately this quota is ridicously low, 1 request per hour per user&amp;hellip; Let&#39;s all hope it doesn&#39;t stay this way :/&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FORCE &amp;gt;= 3.0&lt;/strong&gt; is able to handle LTA data. Previous &lt;strong&gt;FORCE&lt;/strong&gt; versions crash when trying to download offline products.
&lt;strong&gt;FORCE L1AS&lt;/strong&gt; determines whether a product is online or offline.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If online, the image is downloaded as described above.&lt;/li&gt;
&lt;li&gt;If offline, a pull request from offline to online storage is sent. ESA hasn&#39;t implemented any callback for this retrieval, thus &lt;strong&gt;FORCE L1AS&lt;/strong&gt; will simply send pull requests for each requested offline image, probably download some available online images, and then exit. &lt;strong&gt;FORCE L1AS&lt;/strong&gt; needs to be run again to retrieve the restored data. To this end, it comes in handy to set up a download scheduler as desribed above.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-level1-sentinel2 /data/Dagobah/S2L1C /data/Dagobah/S2L1C/zambia.txt &amp;quot;25.43/-12.46,25.94/-12.46,25.94/-11.98,25.39/-11.99,25.43/-12.46&amp;quot; 2018-07-01 2018-07-31 0 50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2020-02-15_15:49:18 - Found 12 S2A/B files.
2020-02-15_15:49:18 - Found 12 S2A/B files on this page.
S2B_MSIL1C_20180730T081559_N0206_R121_T35LLG_20180730T141111.SAFE: Pulling from Long Term Archive. Success. Rerun this program after a while
S2B_MSIL1C_20180727T080609_N0206_R078_T35LLG_20180727T121446.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180725T081601_N0206_R121_T35LLG_20180725T121615.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180722T080611_N0206_R078_T35LLG_20180722T115605.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2B_MSIL1C_20180720T081559_N0206_R121_T35LLG_20180720T121127.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2B_MSIL1C_20180717T080609_N0206_R078_T35LLG_20180717T120239.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180715T081601_N0206_R121_T35LLG_20180715T103432.SAFE: Pulling from Long Term Archive. Failed. You have exhausted your user quota. Rerun this program after a while
S2A_MSIL1C_20180712T080611_N0206_R078_T35LLG_20180712T102334.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2B_MSIL1C_20180710T081559_N0206_R121_T35LLG_20180710T120813.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2B_MSIL1C_20180707T080609_N0206_R078_T35LLG_20180707T115209.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2A_MSIL1C_20180705T081601_N0206_R121_T35LLG_20180705T103349.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
S2A_MSIL1C_20180702T080611_N0206_R078_T35LLG_20180702T115230.SAFE: Pulling from Long Term Archive. Failed. Too Many Requests
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: DEM</title>
      <link>https://davidfrantz.github.io/tutorials/force-dem/dem/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-dem/dem/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will show how to prepare a Digital Elevation Model (DEM) for the &lt;strong&gt;FORCE Level 2 Processing System (FORCE L2PS)&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;FORCE L2PS&lt;/strong&gt; uses a DEM for&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enhanced cloud and cloud shadow detection,&lt;/li&gt;
&lt;li&gt;atmospheric correction, and to&lt;/li&gt;
&lt;li&gt;perform the topographic correction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the cloud shadow detection, the DEM is primarily used to distinguish cloud shadows from water and topographic shadows. In the atmospheric correction, the DEM is used to scale the optical depths with altitude. The topographic correction is of course relying on the DEM. In principle, &lt;strong&gt;FORCE L2PS&lt;/strong&gt; can be used without a DEM (FILE_DEM = NULL). In this case, the surface is assumed to be flat at z = 0m a.s.l. The topographic correction, however, can only be used if a DEM is given (surprise).&lt;/p&gt;
&lt;p&gt;In any case, it is strongly advised to use a DEM. Plus, it is not complicated to acquire it, free options are available. You probably already have a DEM for your study area anyway.&lt;/p&gt;
&lt;h2 id=&#34;data-format&#34;&gt;&lt;strong&gt;Data format&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;There are little requirements on the data format:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The unit must be meters.&lt;/li&gt;
&lt;li&gt;The Nodata value shouldn&#39;t be 0, which is a valid elevation.&lt;/li&gt;
&lt;li&gt;The DEM must cover the complete image(s) to be processed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, a mosaic that covers your complete study area needs to be prepared. The DEM is warped and cropped to the projection and extent of the Level 1 image, which is processed with &lt;strong&gt;FORCE L2PS&lt;/strong&gt;. This is done on-the-fly. Therefore, data type, data format, projection, extent etc. can be chosen freely - as long as GDAL is able to handle it (GDAL can handle pretty much anything).&lt;/p&gt;
&lt;p&gt;Please note, that pixels with nodata values in the DEM will have nodata values in the Level 2 products, too. Thus, make sure your DEM covers the complete area of interest.&lt;/p&gt;
&lt;h2 id=&#34;which-dem&#34;&gt;&lt;strong&gt;Which DEM?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The DEM should match the resolution of the Level 1 image data as closely as possible. If possible, it is advised to use a finer resolution. However, as it is hard to acquire high spatial resolution DEMs, especially for larger areas, lower resolution works too. Often, we use the 30m SRTM DEM or 30m ASTER DEM, or a combination thereof, e.g. SRTM filled with ASTER (SRTM is a bit better, but there are holes in mountainous regions, and coverage is only 60°N-60°S).&lt;/p&gt;
&lt;p&gt;The SRTM DEM can be obtained from &lt;a href=&#34;https://earthexplorer.usgs.gov/&#34;&gt;EarthExplorer&lt;/a&gt;. The ASTER DEM can be obtained from &lt;a href=&#34;https://search.earthdata.nasa.gov/search/&#34;&gt;EarthData&lt;/a&gt; or &lt;a href=&#34;https://ssl.jspacesystems.or.jp/ersdac/GDEM/E/&#34;&gt;Japan Space Systems&lt;/a&gt;. Both are free of charge.&lt;/p&gt;
&lt;h2 id=&#34;prepare-the-mosaic&#34;&gt;&lt;strong&gt;Prepare the mosaic&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The following steps illustrate how to build a virtual mosaic from SRTM data. Generally, DEM data come in tiles (datacube style), e.g. each SRTM tile covers 1°. The &lt;a href=&#34;gdal.org/drivers/raster/vrt.html&#34;&gt;GDAL Virtual Format&lt;/a&gt; allows to mosaick data without producing a physical representation, i.e. the virtual mosaic only holds links to the original tiled data, plus some rules on how to combine them into the mosaic.&lt;/p&gt;
&lt;p&gt;Assuming you have downloaded some SRTM tiles, we first prepare a text file that holds all the filepaths:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;find /data/Dagobah/global/dem/srtm -name &#39;*.tif&#39; &amp;gt; /data/Earth/global/dem/srtm.txt
cat /data/Dagobah/global/dem/srtm.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/global/dem/srtm/n35_e027_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e027_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e026_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n37_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e024_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n35_e025_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e023_1arc_v3.tif
/data/Dagobah/global/dem/srtm/n36_e027_1arc_v3.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we use the &lt;code&gt;gdalbuildvrt&lt;/code&gt; command to generate the virtual mosaic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalbuildvrt -input_file_list /data/Dagobah/global/dem/srtm.txt /data/Earth/global/dem/srtm.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0...10...20...30...40...50...60...70...80...90...100 - done.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The VRT file is a simple xml file:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head -n 14 /data/Dagobah/global/dem/srtm.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;VRTDataset rasterXSize=&amp;quot;18001&amp;quot; rasterYSize=&amp;quot;10801&amp;quot;&amp;gt;
  &amp;lt;SRS&amp;gt;GEOGCS[&amp;quot;WGS 84&amp;quot;,DATUM[&amp;quot;WGS_1984&amp;quot;,SPHEROID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7030&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6326&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4326&amp;quot;]]&amp;lt;/SRS&amp;gt;
  &amp;lt;GeoTransform&amp;gt;  2.2999861111111112e+01,  2.7777777777777794e-04,  0.0000000000000000e+00,  3.8000138888888891e+01,  0.0000000000000000e+00, -2.7777777777777794e-04&amp;lt;/GeoTransform&amp;gt;
  &amp;lt;VRTRasterBand dataType=&amp;quot;Int16&amp;quot; band=&amp;quot;1&amp;quot;&amp;gt;
    &amp;lt;NoDataValue&amp;gt;-32767&amp;lt;/NoDataValue&amp;gt;
    &amp;lt;ColorInterp&amp;gt;Gray&amp;lt;/ColorInterp&amp;gt;
    &amp;lt;ComplexSource&amp;gt;
      &amp;lt;SourceFilename relativeToVRT=&amp;quot;1&amp;quot;&amp;gt;srtm/n35_e027_1arc_v3.tif&amp;lt;/SourceFilename&amp;gt;
      &amp;lt;SourceBand&amp;gt;1&amp;lt;/SourceBand&amp;gt;
      &amp;lt;SourceProperties RasterXSize=&amp;quot;3601&amp;quot; RasterYSize=&amp;quot;3601&amp;quot; DataType=&amp;quot;Int16&amp;quot; BlockXSize=&amp;quot;3601&amp;quot; BlockYSize=&amp;quot;1&amp;quot; /&amp;gt;
      &amp;lt;SrcRect xOff=&amp;quot;0&amp;quot; yOff=&amp;quot;0&amp;quot; xSize=&amp;quot;3601&amp;quot; ySize=&amp;quot;3601&amp;quot; /&amp;gt;
      &amp;lt;DstRect xOff=&amp;quot;14400&amp;quot; yOff=&amp;quot;7200&amp;quot; xSize=&amp;quot;3601&amp;quot; ySize=&amp;quot;3601&amp;quot; /&amp;gt;
      &amp;lt;NODATA&amp;gt;-32767&amp;lt;/NODATA&amp;gt;
    &amp;lt;/ComplexSource&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any software that is based on GDAL is able to read this file, e.g. QGIS - and &lt;strong&gt;FORCE&lt;/strong&gt;. The filepath of this file needs to given in the &lt;strong&gt;FORCE L2PS&lt;/strong&gt; parameter file:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FILE_DEM = /data/Dagobah/global/dem/srtm.vrt&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: The Datacube</title>
      <link>https://davidfrantz.github.io/tutorials/force-datacube/datacube/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-datacube/datacube/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;script src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34; type=&#34;text/javascript&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial explains what a datacube is, how it is parameterized, how you can find a POI, how to visualize the tiling grid, and how to conveniently display cubed data.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;FORCE makes heavy use of the data cube concept. This includes two main points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;All data are in the &lt;strong&gt;same coordinate system&lt;/strong&gt;, which should be valid for a large regional extent (e.g. a continental projection).&lt;/li&gt;
&lt;li&gt;The data are organized in regular, non-overlapping &lt;strong&gt;tiles&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-scheme.jpg&#34; data-caption=&#34;Overview of the datacube concept in FORCE&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-scheme.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overview of the datacube concept in &lt;strong&gt;FORCE&lt;/strong&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The ‘grid’ is the regular spatial subdivision of the land surface in the target coordinate system.&lt;/li&gt;
&lt;li&gt;The ‘grid origin’ is the location, where the tile numbering starts with zero. Tile numbers increase toward the South and East. Although not recommended, negative tile numbers may be present if the tile origin is not North–West of the study area.&lt;/li&gt;
&lt;li&gt;The ‘tile’ is one entity of the grid, i.e., a grid cell with a unique tile identifier, e.g., X0003_Y0002. The tile is stationary, i.e., it always covers the same extent on the land surface.&lt;/li&gt;
&lt;li&gt;The ‘tile size’ is defined in target coordinate system units (most commonly in meters). Tiles are square.&lt;/li&gt;
&lt;li&gt;Each ‘original image’ is partitioned into several ‘chips’, i.e., any original image is intersected with the grid and then tiled into chips.&lt;/li&gt;
&lt;li&gt;Chips are grouped in ‘datasets’, which group data, e.g. according to acquisition date and sensor.&lt;/li&gt;
&lt;li&gt;The ‘data cube’ groups all datasets within a tile in a time-ordered manner. The data cube may contain data from several sensors and different resolutions. Thus, the pixel size is allowed to vary, but the tile extent stays fixed. The tile size must be a multiple of the resolutions. Other data like features or auxiliary data are also permitted in the data cube (e.g. DEM or climate variables).&lt;/li&gt;
&lt;li&gt;The data cube concept allows for non-redundant data storage and efficient data access, as well as simplified extraction of data and information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;how-to-define-the-datacube-parameters&#34;&gt;&lt;strong&gt;How to define the datacube parameters?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;When generating Level 2 ARD data with &lt;strong&gt;FORCE L2PS&lt;/strong&gt;, you need to define the datacube in the parameter file. Empty parameter files can be generated with &lt;code&gt;force-parameter&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DO_REPROJ&lt;/code&gt; indicates whether the images should be reprojected to the target coordinate system - or stay in their original UTM projection.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DO_TILE&lt;/code&gt; indicates whether the images should be tiled to chips that intersect with the grid system - or stay in the original reference system (WRS-2/MGRS).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PROJECTION&lt;/code&gt; defines the target coordinate system. This projection should ideally be valid for a large geographic extent. The projection needs to given as &amp;ldquo;WKT&amp;rdquo; string. You can verify your projection (and convert to WKT from another format) using &lt;code&gt;gdalsrsinfo&lt;/code&gt; (see below). If this fails, you need to fix the projection - otherwise &lt;strong&gt;FORCE L2PS&lt;/strong&gt; will likely fail, too.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ORIGIN_LAT&lt;/code&gt; and &lt;code&gt;ORIGIN_LON&lt;/code&gt; are the origin coordinates of the grid system in decimal degree. The upper left corner of tile X0000_Y0000 represents this point. It is a good choice to use a coordinate that is North-West of your study area – to avoid negative tile numbers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TILE_SIZE&lt;/code&gt; is the tile size (in target units, commonly in meters). Tiles are square.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BLOCK_SIZE&lt;/code&gt; is the block size (in target units, commonly in meters) of the image chips. Blocks are stripes, i.e. they are as wide as the tile and as high as specified here. The blocks represent the internal structure of the GeoTiffs, and represent the primary processing unit of the force-higher-level routines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two default projection / grid systems are predefined in &lt;strong&gt;FORCE&lt;/strong&gt;. They can be specified via the &lt;code&gt;PROJECTION&lt;/code&gt; parameter instead of giving a WKT string. The predefined options have their own settings for &lt;code&gt;ORIGIN_LAT&lt;/code&gt;, &lt;code&gt;ORIGIN_LON&lt;/code&gt;, &lt;code&gt;TILE_SIZE&lt;/code&gt;, and &lt;code&gt;BLOCK_SIZE&lt;/code&gt;, thus the values given in the parameterfile will be ignored. &lt;a href=&#34;https://cartography.tuwien.ac.at/eurocarto/wp-content/uploads/2015/09/3_6_ppt.pdf&#34;&gt;EQUI7&lt;/a&gt; consists of 7 Equi-Distant, continental projections with a tile size of 100km. &lt;a href=&#34;https://measures-glance.github.io/glance-grids/&#34;&gt;GLANCE7&lt;/a&gt; consists of 7 Equal-Area, continental projections, with a tile size of 150km. One datacube will be generated for each continent.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you are not using the datacube options, i.e. &lt;code&gt;DO_REPROJ = FALSE&lt;/code&gt; or &lt;code&gt;DO_TILE = FALSE&lt;/code&gt;, you are running into a &lt;strong&gt;dead end&lt;/strong&gt; for &lt;strong&gt;FORCE&lt;/strong&gt;. In this case, the data cannot be further processed or analysed with any higher level FORCE functionality&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-to-validate-the-projection&#34;&gt;&lt;strong&gt;How to validate the projection?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;GDAL has a built-in projection conversion/validation tool:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gdalsrsinfo -v &#39;PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,GEOGCS[&amp;quot;ETRS89&amp;quot;,DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],UNIT[&amp;quot;metre&amp;quot;,1,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Validate Succeeds

PROJ.4 : &#39;+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs &#39;

OGC WKT :
PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,
    GEOGCS[&amp;quot;ETRS89&amp;quot;,
        DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,
            SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,
                AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],
            TOWGS84[0,0,0,0,0,0,0],
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],
        PRIMEM[&amp;quot;Greenwich&amp;quot;,0,
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],
        UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,
            AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],
        AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],
    PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],
    PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],
    PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],
    PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],
    PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],
    UNIT[&amp;quot;metre&amp;quot;,1,
        AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],
    AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;where-is-the-datacube-definition-stored&#34;&gt;&lt;strong&gt;Where is the datacube definition stored?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At the top level of the generated datacube, a text file will be generated (&lt;code&gt;datacube-definition.prj&lt;/code&gt;). This file is key for all
&lt;strong&gt;FORCE&lt;/strong&gt; higher-level functionality. Each higher-level module will save a copy of this file in the corresponding output directory. If this file is not present, the tools will fail. Therefore, &lt;strong&gt;do not modify, move, or delete this file&lt;/strong&gt;. This file contains the datacube definition as defined above.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;projection in WKT&lt;/li&gt;
&lt;li&gt;grid origin, longitude&lt;/li&gt;
&lt;li&gt;grid origin, latitude&lt;/li&gt;
&lt;li&gt;grid origin, x-coordinate in projection&lt;/li&gt;
&lt;li&gt;grid origin, y-coordinate in projection&lt;/li&gt;
&lt;li&gt;tile size in projection units&lt;/li&gt;
&lt;li&gt;block size in projection units&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat /data/Dagobah/edc/level2/datacube-definition.prj
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;PROJCS[&amp;quot;ETRS89 / LAEA Europe&amp;quot;,GEOGCS[&amp;quot;ETRS89&amp;quot;,DATUM[&amp;quot;European_Terrestrial_Reference_System_1989&amp;quot;,SPHEROID[&amp;quot;GRS 1980&amp;quot;,6378137,298.257222101,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7019&amp;quot;]],TOWGS84[0,0,0,0,0,0,0],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6258&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;8901&amp;quot;]],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9122&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4258&amp;quot;]],PROJECTION[&amp;quot;Lambert_Azimuthal_Equal_Area&amp;quot;],PARAMETER[&amp;quot;latitude_of_center&amp;quot;,52],PARAMETER[&amp;quot;longitude_of_center&amp;quot;,10],PARAMETER[&amp;quot;false_easting&amp;quot;,4321000],PARAMETER[&amp;quot;false_northing&amp;quot;,3210000],UNIT[&amp;quot;metre&amp;quot;,1,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;9001&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;3035&amp;quot;]]
-25.000000
60.000000
2456026.250000
4574919.500000
30000.000000
3000.0000000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;In some rare circumstances, you might need to generate this file on your own. However, this only applies if - for any reason - you skip the Level 2 processing (e.g. if you only want to work with external features, or trick &lt;strong&gt;FORCE&lt;/strong&gt; into using external ARD datasets).&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how-is-the-datacube-organized&#34;&gt;&lt;strong&gt;How is the datacube organized?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In practice, the tiles are directories in the file system, and each chip represents one file.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2 | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;X0134_Y0095
X0134_Y0096
X0134_Y0097
X0134_Y0098
X0134_Y0099
X0135_Y0095
X0135_Y0096
X0135_Y0097
X0135_Y0098
X0135_Y0099
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X0134_Y0097/*.tif | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_VZN.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181230_LEVEL2_SEN2B_VZN.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within the tile, &lt;strong&gt;FORCE&lt;/strong&gt; semantically groups files into datasets if they have the same sensor and date (e.g. multiple products like Bottom-of-Atmosphere reflectance &lt;code&gt;BOA&lt;/code&gt; and Quality Assurance Information &lt;code&gt;QAI&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_*.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_CLD.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_HOT.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_QAI.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_VZN.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note that chips in different tiles have the same filename, thus they can easily be mosaicked.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/X*/20181225_LEVEL2_SEN2A_BOA.tif | wc -l
ls /data/Dagobah/edc/level2/X*/20181225_LEVEL2_SEN2A_BOA.tif | tail
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;49
/data/Dagobah/edc/level2/X0133_Y0100/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0096/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0098/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0134_Y0099/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0095/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0096/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0097/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0098/20181225_LEVEL2_SEN2A_BOA.tif
/data/Dagobah/edc/level2/X0135_Y0099/20181225_LEVEL2_SEN2A_BOA.tif
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;i-processed-quite-some-data-there-are-many-many-tiles-how-do-i-find-a-poi&#34;&gt;&lt;strong&gt;I processed quite some data. There are many, many tiles. How do I find a POI?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Given any coordinate \((\lambda,\phi)\), the computation of the corresponding tile is pretty straightforward.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Convert the coordinate \((\lambda,\phi)\) to the projected coordinate \((X,Y)\)&lt;/li&gt;
&lt;li&gt;Given the tile size \(t_s\) and the grid origin in projected coordinates \((X_O,Y_O)\), the tile ID can be computed as \(Tile_X = floor((X-X_O)/t_s)\) and \(Tile_Y = floor((Y_O-Y)/t_s)\)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With some more math, you can also compute the exact pixel.&lt;/p&gt;
&lt;p&gt;However, there is also a &lt;strong&gt;FORCE&lt;/strong&gt; program that relieves you from doing this on your own:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tile-finder
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-tile-finder datacube lon lat res
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tile-finder /data/Dagobah/edc/level2 13.404194 52.502889 10
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Point { LON/LAT (13.40,52.50) | X/Y (4552071.50,3271363.25) }
  is in tile X0069_Y0043 at pixel 2604/1355
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another useful &lt;strong&gt;FORCE&lt;/strong&gt; program can generate a vector file (shapefile or kml) for convenient display of the tiles.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tabulate-grid
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-tabulate-grid datacube bottom top left right format
             format: shp or kml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-tabulate-grid /data/Dagobah/edc/level2 35 60 0 20 kml
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/datacube-grid.kml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The grid can easily be loaded in GoogleEarth or any GIS. The attribute table contains the tile ID.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-google-grid.jpg&#34; data-caption=&#34;Exported grid loaded in Google Earth&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-google-grid.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Exported grid loaded in Google Earth
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;how-to-visualize-data-for-a-large-extent-more-conveniently&#34;&gt;&lt;strong&gt;How to visualize data for a large extent more conveniently?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Whenever you use a FORCE routine, cubed data will be generated. It is a bit cumbersome to display such data for a large extent without some further treatment. The following recipe can be used for any cubed &lt;strong&gt;FORCE&lt;/strong&gt; data - irrespective of processing level.&lt;/p&gt;
&lt;p&gt;Lucky us, the &lt;a href=&#34;https://gdal.org/drivers/raster/vrt.html&#34;&gt;GDAL virtual format&lt;/a&gt; represents an ideal concept for this. With VRTs, mosaicks of cubed data can be generated without physically copying the data. The VRT is basically a text file in xml-Format, which both holds (relative) links to the original data and the rules to assemble the mosaic on-the-fly.
&lt;strong&gt;FORCE&lt;/strong&gt; comes with a tool to generate such mosaics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-mosaic tiled-archive
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-mosaic /data/Dagobah/edc/level2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;force-mosaic searches for image files in the datacube, and mosaics all files with the same basename. The mosaics are stored in the &lt;code&gt;mosaic&lt;/code&gt; subdirectory.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic | head
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;19840328_LEVEL2_LND05_BOA.vrt
19840328_LEVEL2_LND05_CLD.vrt
19840328_LEVEL2_LND05_HOT.vrt
19840328_LEVEL2_LND05_QAI.vrt
19840328_LEVEL2_LND05_VZN.vrt
19840409_LEVEL2_LND05_BOA.vrt
19840409_LEVEL2_LND05_CLD.vrt
19840409_LEVEL2_LND05_HOT.vrt
19840409_LEVEL2_LND05_QAI.vrt
19840409_LEVEL2_LND05_VZN.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To speed up visualization, pyramids might be generated for the VRT files. This significantly increases loading and response times for visualization. However, pyramid layers are basically copies of the original data at reduced resolution, and as such, they consume some disc space. Consider from case to case whether fast display merits the excess disc usage. &lt;strong&gt;FORCE&lt;/strong&gt; comes with a tool to generate pyramids:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-pyramid
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-pyramid file
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pyramids for one file can be generated with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-pyramid /data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
computing pyramids for 19840828_LEVEL2_LND05_BOA.vrt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Practically, a DEFLATE compressed overview image will be stored next to the VRT:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt
/data/Dagobah/edc/level2/mosaic/19840828_LEVEL2_LND05_BOA.vrt.ovr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pyramids for all VRT mosaics can be parallely generated with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Dagobah/edc/level2/mosaic/*.vrt | parallel force-pyramid {}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Any modern software based on GDAL (e.g. QGIS) is able to display VRTs, and can also handle the attached pyramid layers. Mosaicking is done on-the-fly, data outside of the display extent are not loaded.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-datacube-mosaic.jpg&#34; data-caption=&#34;VRT mosaick loaded in QGIS&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-datacube-mosaic.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    VRT mosaick loaded in QGIS
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Quality Bits a.k.a. Cloud Masks etc.</title>
      <link>https://davidfrantz.github.io/tutorials/force-qai/qai/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-qai/qai/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will explain what quality bits are, how quality bits are implemented in &lt;strong&gt;FORCE&lt;/strong&gt;, how to visualize them, and how to deal with them in Higher Level Processing.&lt;/p&gt;
&lt;h2 id=&#34;what-are-quality-bits&#34;&gt;&lt;strong&gt;What are quality bits?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;FORCE L2PS&lt;/strong&gt; provides a description of the quality of each pixel in the form of quality bits. This bit-packed information allows users to apply per pixel filters to all Level 2 products. The bits represent combinations of surface, atmospheric, and processing-related conditions that can affect the overall usefulness of a given pixel for a particular application. The success of any follow-up analysis depends on the rigorous usage of these information!
A good explanation of quality bits is given by the &lt;a href=&#34;https://www.usgs.gov/land-resources/nli/landsat/landsat-collection-1-level-1-quality-assessment-band?qt-science_support_page_related_con=0#qt-science_support_page_related_con&#34;&gt;USGS&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The bit-packed information in the QA bands is a translation of binary strings. For example, the integer value “1” translates to the binary value “0001.” The binary value “0001” has 4 bits, written right to left as bits 0 (“1”), 1 (“0”), 2 (“0”), and 3 (“0”). Each of the bits 0-3 represents a condition that can affect the calculation of a physical value. [&amp;hellip;] If the condition is true, the bit is set to “1,” or “0” if false.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;sounds-complicated-why-not-use-a-scene-classification&#34;&gt;&lt;strong&gt;Sounds complicated… Why not use a scene classification?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Although interpretation of quality bits is not immediate to humans, they do possess quite some advantages. As opposed to a scene classification, quality bits allow the flagging of multiple conditions, e.g. ice clouds, cloud shadows on top of clouds or snow, high aerosol load and cloud, etc. If a 16bit Integer is used for storing the quality bits, up to 16 different conditions can co-exist in any possible combination. In a scene classification, only one condition can be stored, and the algorithm developer needs to make assumptions on the priority of the conditions; however these may differ from application to application. Quality bits allow to store all these information in a single  image. From a technical perspective, quality bits save disc space, and reduce the I/O load for follow-up analyses.&lt;/p&gt;
&lt;h2 id=&#34;quality-bits-in-force&#34;&gt;&lt;strong&gt;Quality bits in FORCE&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;strong&gt;FORCE&lt;/strong&gt;, the quality bits are found in the Quality Assurance Information (&lt;code&gt;QAI&lt;/code&gt;) product, which is an integral part of each Level 2 dataset, and is alway present next to the reflectance images (&lt;code&gt;BOA&lt;/code&gt; or &lt;code&gt;TOA&lt;/code&gt;).
When generating Best Available Pixel (&lt;code&gt;BAP&lt;/code&gt;) composites (Level 3), the bit flags of the selected observation are stored in the first band of the composite information (&lt;code&gt;INF&lt;/code&gt;) product.
Currently &lt;strong&gt;FORCE&lt;/strong&gt; implements a 16bit QAI layer with 12 quality bits, some of them as double-bit words:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Bit No.&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Parameter name&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Bit comb.&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Integer&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;State&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Valid data&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;valid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;1–2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cloud state&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;clear&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;less confident cloud (i.e., buffered cloud 300 m)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;confident, opaque cloud&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;cirrus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Cloud shadow flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Snow flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Water flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;6–7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aerosol state&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;estimated (best quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;interpolated (mid quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;high (aerosol optical depth &amp;gt; 0.6, use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;fill (global fallback, low quality)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Subzero flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes (use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Saturation flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes (use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;High sun zenith flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes (sun elevation &amp;lt; 15°, use with caution)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;11–12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Illumination state&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;00&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;good (incidence angle &amp;lt; 55°, best quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;medium (incidence angle 55°–80°, good quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;poor (incidence angle &amp;gt; 80°, low quality for top. correction)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;shadow (incidence angle &amp;gt; 90°, no top. correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Slope flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;no (cosine correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;yes (enhanced C-correction applied)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Water vapor flag&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;measured (best quality, only Sentinel-2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;fill (scene average, only Sentinel-2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Empty&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Nodata values are values where nothing was observed, where auxiliary data was not given (e.g. nodata in DEM), or
where data is substantially corrupt (e.g. impulse noise, or when the surface reflectance estimate is &amp;gt; 2.0 or &amp;lt; -1.0)&lt;/li&gt;
&lt;li&gt;Clouds are given in three categories, i.e. opaque clouds (confident cloud), buffered clouds (300m; less confident cloud), and cirrus clouds.&lt;/li&gt;
&lt;li&gt;Cloud shadows are detected on the basis of the cloud layer. If a cloud is missed, the cloud shadow is missed, too. If a false
positive cloud is detected, false positive cloud shadows follow.&lt;/li&gt;
&lt;li&gt;Aerosol Optical Depth is estimated for fairly coarse grid cells. If there is no valid AOD estimation in any cell, values are
interpolated. If there is no valid AOD estimation for the complete image, a fill value is assigned (AOD is guessed). If AOD @550nm is higher than 0.6, it is flagged as high aerosol; this is not necessarily critical, but should be used with caution (see subzero flag).&lt;/li&gt;
&lt;li&gt;If the surface reflectance estimate in any band is &amp;lt; 0, the subzero flag is set. This can point to overestimation of AOD.&lt;/li&gt;
&lt;li&gt;If DNs were saturated, or if the surface reflectance estimate in any band is &amp;gt; 1, the saturation flag is set.&lt;/li&gt;
&lt;li&gt;If sun elevation is smaller than 15°, the high sun zenith flag is set. Use this data with caution, radiative transfer computations might be out of specification.&lt;/li&gt;
&lt;li&gt;The illumination state is related to the quality of the topographic correction. If the incidence angle is smaller than 55°, quality is best. If the incidence angle is larger than 80°, the quality of the topographic correction is low, and data artefacts are possible. If the area is not illuminated at all, no topographic correction is done (values are the same as without topographic correction).&lt;/li&gt;
&lt;li&gt;The slope flag indicates whether a simple cosine correction (slope ≤ 2°) was used for topographic correction, or if the enhanced C-correction was used (slope &amp;gt; 2°).&lt;/li&gt;
&lt;li&gt;The water vapor flag indicates whether water vapor was estimated, or if the scene average was used to fill. Water vapor is not estimated over water and cloud shadow pixels. This flag only applies to Sentinel-2 images.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;visualization&#34;&gt;&lt;strong&gt;Visualization&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visualizing the raw QAI image is pretty meaningless. Don’t be surprised that the integers do not resemble any of the patterns you would expect (e.g. cloud distribution).&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-boa.jpg&#34; data-caption=&#34;Sentinel-2B image over Berlin, 01.07.2019; left: RGB image; right: quality bits&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-boa.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sentinel-2B image over Berlin, 01.07.2019; left: RGB image; right: quality bits
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;quicklooks&#34;&gt;&lt;strong&gt;Quicklooks&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Since v. 3.0, &lt;strong&gt;FORCE L2PS&lt;/strong&gt; can output quicklook images for each Level 2 dataset (&lt;code&gt;OVV&lt;/code&gt; = overview product). These thumbnails serve as first impression of image quality. Some of the quality conditions are superimposed on the RGB images. Opaque clouds are shown in pink, cirrus clouds in red, cloud shadows in cyan, snow in yellow, saturated pixels in orange, and sub-zero reflectance values in a greenish tone. The overview for the QAI image from above is shown here:&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-ovv.jpg&#34; data-caption=&#34;Quicklook image generated by FORCE L2PS; pink: opaque clouds; cyan: cloud shadows&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-ovv.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Quicklook image generated by &lt;strong&gt;FORCE L2PS&lt;/strong&gt;; pink: opaque clouds; cyan: cloud shadows
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;inflate-quality-bits&#34;&gt;&lt;strong&gt;Inflate quality bits&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A full deciphering of all quality bits to individual quality masks can be generated with &lt;strong&gt;FORCE&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-qai-inflate
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Usage: force-qai-inflate QAI dir format
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-qai-inflate /data/level2/X0069_Y0043/20190701_LEVEL2_SEN2B_QAI.tif ~/temp GTiff
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This program generates a 12-band image, where each of the flags (see table above) is written to a separate band. However, force-qai-inflate was not designed to generate inflated masks for each and every Level 2 product in a routine manner due to the computational and disc-space related overhead. We strongly recommend to make use of  the bits directly (see remaining part of the tutorial).&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-cld.jpg&#34; data-caption=&#34;Quality bits; left: cloud state; right: cloud shadow flag&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-cld.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Quality bits; left: cloud state; right: cloud shadow flag
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;quality-bit-rendering-in-qgis&#34;&gt;&lt;strong&gt;Quality bit rendering in QGIS&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;There is a nice QGIS plugin from my colleague &lt;a href=&#34;https://www.geographie.hu-berlin.de/en/professorships/eol/people/labmembers/benjamin_jakimow&#34;&gt;Benjamin Jakimow&lt;/a&gt;, which can visualize quality bits in QGIS &lt;em&gt;on the fly&lt;/em&gt;. Quality bit inflating is not necessary anymore! The &lt;a href=&#34;http://plugins.qgis.org/plugins/BitFlagRenderer/&#34;&gt;Bit Flag Renderer plugin&lt;/a&gt; provides a new renderer for QGIS, with which any quality bit product can flexibly be visualized. The plugin includes predefined bit visualization rules for the &lt;strong&gt;FORCE&lt;/strong&gt; QAI bits. The default visualization matches the information and colors from the quicklook images described above):&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-bfr.jpg&#34; data-caption=&#34;Bit Flag Renderer in QGIS displaying a quality bit layer on-the-fly with the pre-defined FORCE settings&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-bfr.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Bit Flag Renderer in QGIS displaying a quality bit layer on-the-fly with the pre-defined FORCE settings
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;quality-masking-in-higher-level-processing&#34;&gt;&lt;strong&gt;Quality masking in Higher Level Processing&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For follow-up processing and analyses, the usage of the QAI information is key, e.g. to exclude clouds. In all &lt;strong&gt;FORCE Higher Level routines&lt;/strong&gt;, quality masking is done on the fly, and the user has full control about what condition(s) to filter. The parameter &lt;code&gt;SCREEN_QAI&lt;/code&gt; provides a simple mechanism to mask out any combination of conditions using any of the following keywords: &lt;em&gt;NODATA, CLOUD_OPAQUE, CLOUD_BUFFER, CLOUD_CIRRUS, CLOUD_SHADOW, SNOW, WATER, AOD_FILL, AOD_HIGH, AOD_INT, SUBZERO, SATURATION, SUN_LOW, ILLUMIN_NONE, ILLUMIN_POOR, ILLUMIN_LOW, SLOPED, WVP_NONE&lt;/em&gt;. The default parametrization is to filter out nodata, clouds, cloud shadows, snow, saturated or subzero reflectance:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SCREEN_QAI = NODATA CLOUD_OPAQUE CLOUD_BUFFER CLOUD_CIRRUS CLOUD_SHADOW SNOW SUBZERO SATURATION&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Following images illustrate the effect of quality filtering on an average reflectance image generated by using all available observations over a 3 month period (using Spectral Temporal Metrics in the &lt;strong&gt;Time Series Analysis module&lt;/strong&gt;). The left image was produced by filtering nodata values only, the right image was produced using the default quality screening.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/tutorial-qai-avg.jpg&#34; data-caption=&#34;Average reflectance over three month; left: not using quality bits; right with quality bits&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/tutorial-qai-avg.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Average reflectance over three month; left: &lt;strong&gt;not using&lt;/strong&gt; quality bits; right &lt;strong&gt;with&lt;/strong&gt; quality bits
  &lt;/figcaption&gt;


&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>FORCE Tutorial: Water Vapor Database</title>
      <link>https://davidfrantz.github.io/tutorials/force-wvdb/wvdb/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://davidfrantz.github.io/tutorials/force-wvdb/wvdb/</guid>
      <description>&lt;p&gt;&lt;em&gt;This tutorial uses FORCE v. 3.0&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;learning-objective&#34;&gt;&lt;strong&gt;Learning Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This tutorial will show how to prepare the Water Vapor Database (WVDB) for the &lt;strong&gt;FORCE Level 2 Processing System (FORCE L2PS)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Chart start 1&lt;/p&gt;



  


&lt;div id=&#34;chart-179286345&#34; class=&#34;chart pb-3&#34; style=&#34;max-width: 100%; margin: auto;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;/img/test.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-179286345&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();

&lt;/script&gt;
&lt;p&gt;Chart end 1&lt;/p&gt;
&lt;p&gt;Chart start 2&lt;/p&gt;



  


&lt;div id=&#34;chart-674953182&#34; class=&#34;chart pb-3&#34; style=&#34;max-width: 100%; margin: auto;&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        return;
      }
      clearInterval( a );

      Plotly.d3.json(&#34;/img/network.json&#34;, function(chart) {
        Plotly.plot(&#39;chart-674953182&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();

&lt;/script&gt;
&lt;p&gt;Chart end 2&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;During atmospheric correction, the effect of water vapor absorption can only be corrected if we know the amount of water vapor in the atmosphere.&lt;/p&gt;
&lt;p&gt;If you are using Sentinel-2 data only, you can stop reading. Sentinel-2 is equipped with a water vapor channel, and thus, water wapor amount can be estimated from the images.&lt;/p&gt;
&lt;p&gt;Landsat, however, doesn&#39;t have such a band. Therefore, we need to rely on external data, which needs to be precompiled into a water vapor database.&lt;/p&gt;
&lt;h2 id=&#34;water-vapor-database&#34;&gt;&lt;strong&gt;Water Vapor Database&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The database holds water vapor values for the central coordinates of each WRS-2 frame. If available, day-specific values are used.&lt;/p&gt;
&lt;p&gt;The database consists of one table for each day (&lt;code&gt;WVP_YYYY-MM-DD.txt&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Earth/global/wvp/wvdb/WVP_2010-07-*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Earth/global/wvp/wvdb/WVP_2010-07-01.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-02.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-03.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-04.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-05.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-06.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-07.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-08.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-09.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-10.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-11.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-12.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-13.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-14.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-15.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-16.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-17.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-18.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-19.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-20.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-21.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-22.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-23.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-24.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-25.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-27.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-28.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-29.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-30.txt
/data/Earth/global/wvp/wvdb/WVP_2010-07-31.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each file includes one value per coordinate. In the example below, there are 13281 coordinates in each file (global land coverage). The coordinate, which is closest to the center of the Landsat image is selected, and the atmospheric correction uses this value to account for gaseous absorption.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/WVP_2010-07-26.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.3934 80.7603 1.170018 MOD
-22.8654 80.0056 9999.000000 TBD
-29.2236 79.1137 9999.000000 TBD
-34.5930 78.1151 0.614454 MOD
-39.1269 77.0343 0.448552 MOD
-42.9718 75.8898 0.260607 MOD
-46.2552 74.6958 0.282855 MYD
-49.0816 73.4629 0.337015 MOD
-51.5357 72.1989 9999.000000 TBD
-53.6847 70.9100 9999.000000 TBD
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;climatology&#34;&gt;&lt;strong&gt;Climatology&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;If day-specific values are not available (no table is existing, or there is a fill value), a monthly long-term climatology is used instead. The climatology consists of one table for each month (&lt;code&gt;WVP_0000-MM-00.txt&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ls /data/Earth/global/wvp/wvdb/WVP_0000*
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;/data/Earth/global/wvp/wvdb/WVP_0000-01-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-02-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-03-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-04-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-05-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-06-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-08-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-09-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-10-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-11-00.txt
/data/Earth/global/wvp/wvdb/WVP_0000-12-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, each file includes one value per coordinate. The file holds the long-term average, long-term standard deviation, and the number of measurements used to compute these statistics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/WVP_0000-07-00.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.3934 80.7603 1.177704 0.364894 300
-22.8654 80.0056 1.079682 0.328948 311
-29.2236 79.1137 0.695211 0.234917 383
-34.5930 78.1151 0.549352 0.256754 445
-39.1269 77.0343 0.472883 0.224957 480
-42.9718 75.8898 0.410826 0.211346 476
-46.2552 74.6958 0.384219 0.145523 457
-49.0816 73.4629 0.415261 0.170940 456
-51.5357 72.1989 0.515858 0.223122 422
-53.6847 70.9100 0.546611 0.273735 276
&lt;/code&gt;&lt;/pre&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://davidfrantz.github.io/img/wvdb.gif&#34; data-caption=&#34;Global animation of the climatology (monthly average)&#34;&gt;
&lt;img src=&#34;https://davidfrantz.github.io/img/wvdb.gif&#34; alt=&#34;&#34; width=&#34;750&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Global animation of the climatology (monthly average)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;uncertainty-of-the-climatology&#34;&gt;&lt;strong&gt;Uncertainty of the climatology&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The uncertainty of using the climatology was assessed in this paper:
Frantz, D., Stellmes, M., &amp;amp; Hostert, P. (2019). A Global MODIS Water Vapor Database for the Operational Atmospheric Correction of Historic and Recent Landsat Imagery. Remote Sensing, 11, 257. &lt;a href=&#34;https://doi.org/10.3390/rs11030257&#34;&gt;https://doi.org/10.3390/rs11030257&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;prepare-the-wvdb&#34;&gt;&lt;strong&gt;Prepare the WVDB&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We generally use a WVDB generated from MODIS water vapor products (&lt;a href=&#34;https://modis.gsfc.nasa.gov/data/dataprod/mod05.php&#34;&gt;MOD05 and MYD05&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;download-the-ready-to-go-global-wvdb&#34;&gt;&lt;strong&gt;Download the ready-to-go global WVDB&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;You should start by downloading the pre-compiled WVDB with global coverage from &lt;a href=&#34;doi.pangaea.de/10.1594/PANGAEA.893109&#34;&gt;here&lt;/a&gt;. This saves you a lot of processing. This freely available dataset was generated with the &lt;strong&gt;FORCE WVDB&lt;/strong&gt; component, and is comprised of daily global water vapor data for February 2000 to July 2018 for each land-intersecting WRS-2 scene (13281 coordinates), as well as a monthly climatology that can be used if no daily value is available.&lt;/p&gt;
&lt;h3 id=&#34;generate-the-wvdb-on-your-own&#34;&gt;&lt;strong&gt;Generate the WVDB on your own&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;We try to update this dataset in regular intervals. However, if you are in need of more up-to-date data, you can use the &lt;strong&gt;FORCE WVDB&lt;/strong&gt; component to generate/update these tables on your own.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note that you need access to the LAADS DAAC before using this tool (see last section on this page).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FORCE WVDB&lt;/strong&gt; needs a table with input coordinates (center coordinates of WRS-2 frames). The &lt;a href=&#34;doi.pangaea.de/10.1594/PANGAEA.893109&#34;&gt;pre-compiled dataset&lt;/a&gt; includes such a table. If you are not interested in global coverage, you can subset this file. The file should contain two columns separated by white space, and no header. The first column should give the longitude (X), the second column the latitude (Y) with coordinates in decimal degree (negative values for West/South). Any other column is ignored (in the example below, the WRS-2 Path/Row is in the third column).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wc -l /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;13281 /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;head /data/Earth/global/wvp/wvdb/wrs-2-land.coo
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;-15.39340494140 80.76026666750 013001
-22.86543244600 80.00558606640 013002
-29.22356065160 79.11366800820 013003
-34.59295680040 78.11513723200 013004
-39.12687451150 77.03430642440 013005
-42.97184515330 75.88984431700 013006
-46.25519224080 74.69581438230 013007
-49.08160498390 73.46286239410 013008
-51.53569902300 72.19888348300 013009
-53.68466715610 70.91003752470 013010
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;FORCE WVDB&lt;/strong&gt; downloads each Terra/Aqua granule (collection 6.1) that intersects with any of these coordinates. The files are downloaded from the Level1 and Atmosphere Archive and Distribution System (&lt;a href=&#34;ladsweb.modaps.eosdis.nasa.gov&#34;&gt;LAADS&lt;/a&gt;) at NASA’s Goddard Space Flight Center. Note that any permanent or temporary change/shutdown/decommissioning on LAADS’ or MODIS’ end may result in the nonfunctioning of &lt;strong&gt;FORCE WVDB&lt;/strong&gt;&amp;hellip; Also note, that they perform a weekly maintenance, during which their servers are not accessable.&lt;/p&gt;
&lt;p&gt;As with any other FORCE program, you can display short usage instructions by executing the program without any parameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;usage: force-lut-modis coords dir-wvp dir-geometa dir-eoshdf
           [start-year start-month start-day
            end-year   end-month   end-day]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A coordinate file needs to be given as 1st argument.&lt;/p&gt;
&lt;p&gt;The MODIS data are downloaded to dir-eoshdf (this directory must exist). MODIS data that are already in dir-eoshdf are not downloaded again. &lt;em&gt;If the tool crashes because a dataset is corrupt, it is necessary to manually delete this file and run the tool again. Unfortunately, this happens from time to time due to incomplete downloads or if LAADS is unresponsive. The program attempts to re-download a corrupt file up to 10 times, but this error can occur nonetheless.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MOD05/MYD05 data are swath products, and MOD03/MYD03 geometa tables are necessary to relate coordinates to MODIS granules. The geometa tables are downloaded to dir-geometa (this directory must exist). Tables that are already in dir-geometa are not downloaded again. &lt;em&gt;If the tool crashes because a table is invalid, it is necessary to manually delete this file and run the tool again. Unfortunately, this happens from time to time due to incomplete downloads or if LAADS is unresponsive. The program attempts to re-download a corrupt file up to 10 times, but this error can occur nonetheless.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The final water vapor tables are saved in dir-wvp (this directory must exist). Tables that are already in dir-wvp are not processed again (i.e. no download of geometa tables and hdf files).&lt;/p&gt;
&lt;p&gt;The start and end arguments are optional and may be used for parallelization. If they are not given, &lt;strong&gt;FORCE WVDB&lt;/strong&gt; will download the entire time series of all coordinates provided (this can be a lot!).&lt;/p&gt;
&lt;p&gt;This directory is the directory, to which DIR_WVPLUT in the &lt;strong&gt;FORCE L2PS&lt;/strong&gt; parameter file should refer.
&lt;code&gt;DIR_WVPLUT = /data/Earth/global/wvp/wvdb&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If you have finished compiling the WVDB, you may delete the MODIS *.hdf files.&lt;/p&gt;
&lt;p&gt;Download the entire data record (in one process - this is slow):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Download one week:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf 2010 07 01 2010 07 07
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use GNU parallel to download an entire month in 31 parallel processes. This works by creating a list 1..31, which is distributed to 31 jobs. Each job calls &lt;strong&gt;FORCE WVDB&lt;/strong&gt; for one specific day in July 2010. The curly braces are replaced with the list value given to each process.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;seq -w 1 31 | parallel -j31 force-lut-modis /data/Earth/global/wvp/wvdb/wrs-2-land.coo /data/Earth/global/wvp/wvdb /data/Earth/global/wvp/geo /data/Earth/global/wvp/hdf 2010 07 {} 2010 07 {}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;get-access-to-the-laads-daac-edit-13022020&#34;&gt;&lt;strong&gt;Get access to the LAADS DAAC&lt;/strong&gt; &lt;em&gt;(edit 13.02.2020)&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;You need authentification to download data from the LAADS DAAC. This works by requesting an App Key from &lt;a href=&#34;https://ladsweb.modaps.eosdis.nasa.gov/tools-and-services/data-download-scripts/#requesting&#34;&gt;NASA Earthdata&lt;/a&gt;. You can make this key available to &lt;strong&gt;FORCE&lt;/strong&gt; by putting the character string in a file &lt;code&gt;.laads&lt;/code&gt; in your home directory. With this, you should be able to download data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
